<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.58.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>the MaD Seminar &middot; Math and Data</title>

  
  <link type="text/css" rel="stylesheet" href="http://mad.cds.nyu.edu/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="http://mad.cds.nyu.edu/css/poole.css">
  <link type="text/css" rel="stylesheet" href="http://mad.cds.nyu.edu/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="http://mad.cds.nyu.edu/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="http://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="http://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
      
         
                <li>
                    <a href="/about/">
                        
                        <span>About</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/corefaculty/">
                        
                        <span>Faculty</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/phds/">
                        
                        <span>PhD/MsC Students</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/postdocs/">
                        
                        <span>Postdocs and Fellows</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/reading/">
                        
                        <span>Reading Groups</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/seminar/">
                        
                        <span>the MaD Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/micsem/">
                        
                        <span>the MIC Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/miscpeople/">
                        
                        <span>Visitors and Alumni</span>
                    </a>
                </li>
        
      
    </ul>

  </div>
</div>



    <main class="content container">
    <div class="post">
  <h1>the MaD Seminar</h1>
  

<p>The MaD seminar features leading specialists at the interface
of Applied Mathematics, Statistics and Machine Learning. It is partly supported by the Moore-Sloan Data Science Environment at NYU.</p>

<p><strong>Room:</strong> Auditorium Hall 150, Center for Data Science, NYU, <a href="https://www.google.com/maps/place/NYU+Center+for+Data+Science/@40.735016,-73.9969907,17z/data=!3m1!4b1!4m5!3m4!1s0x89c2599787834ad9:0x5dd8af15d9fbc8a3!8m2!3d40.735016!4d-73.994802">60 5th ave</a>.</p>

<p><strong>Time:</strong> 2:00pm-3:00pm, Reception will follow.</p>

<p><strong>Subscribe to the Seminar Mailing list <a href="http://cims.nyu.edu/mailman/listinfo/mad">here</a></strong></p>

<h3 id="schedule-with-confirmed-speakers">Schedule with Confirmed Speakers</h3>

<table>
<thead>
<tr>
<th>Date</th>
<th align="center">Speaker</th>
<th align="center">Title</th>
</tr>
</thead>

<tbody>
<tr>
<td>Sep 5</td>
<td align="center"><a href="https://people.math.osu.edu/memolitechera.1/">Facundo Memoli (OSU)</a></td>
<td align="center"><a href="#facundo">Gromov-Wasserstein distances and distributional invariants of datasets</a></td>
</tr>

<tr>
<td>Sep 19</td>
<td align="center"><a href="https://math.osu.edu/people/mixon.23">Dustin Mixon (OSU)</a></td>
<td align="center"><a href="#dustin">SqueezeFit: Label aware dimensionality reduction via semidefinite programming</a></td>
</tr>

<tr>
<td>Sep 26</td>
<td align="center"><a href="https://people.orie.cornell.edu/mru8/">Madeleine Udell (Cornell)</a></td>
<td align="center"><a href="#udell">Optimal storage SDP</a></td>
</tr>

<tr>
<td>Oct 3</td>
<td align="center"><a href="http://www.mit.edu/~rahulmaz/">Rahul Mazumder (MIT)</a></td>
<td align="center"><a href="#rahul">Learning Structured Sparse Problems at Scale: Continuous and Mixed Integer Programming Perspectives</a></td>
</tr>

<tr>
<td>Oct 10</td>
<td align="center"><a href="https://sites.google.com/view/milanfarhome/">Peyman Milanfar (Google Research)</a></td>
<td align="center"></td>
</tr>

<tr>
<td>Oct 17</td>
<td align="center"></td>
<td align="center"></td>
</tr>

<tr>
<td>Oct 24</td>
<td align="center"><a href="https://homepages.cae.wisc.edu/~loh/">Po-Ling Loh (UW-Madison)</a></td>
<td align="center"></td>
</tr>

<tr>
<td>Oct 31</td>
<td align="center"></td>
<td align="center"></td>
</tr>

<tr>
<td>Nov 7</td>
<td align="center"><a href="http://www.cs.columbia.edu/~blei/">David Blei (Columbia)</a></td>
<td align="center"></td>
</tr>

<tr>
<td>Nov 14</td>
<td align="center"><a href="https://marcocuturi.net">Marco Cuturi (Google Brain)</a></td>
<td align="center"></td>
</tr>

<tr>
<td>Nov 21</td>
<td align="center"><a href="http://math.mit.edu/~elmos/">Elchanan Mossel (MIT)</a></td>
<td align="center"></td>
</tr>

<tr>
<td>Dec 5</td>
<td align="center"><a href="https://lchizat.github.io">Lenaic Chizat (Orsay-Paris Sud)</a></td>
<td align="center"></td>
</tr>
</tbody>
</table>

<hr />

<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2019/">Schedule Spring 2019</a></p>

<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2018/">Schedule Fall 2018</a></p>

<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2018/">Schedule Spring 2018</a></p>

<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2017/">Schedule Fall 2017</a></p>

<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2017/">Schedule Spring 2017</a></p>

<hr />

<h3 id="abstracts">Abstracts</h3>

<h4 id="a-name-rahul-a-rahul-mazumder-learning-structured-sparse-problems-at-scale-continuous-and-mixed-integer-programming-perspectives"><a name="rahul"></a> Rahul Mazumder: Learning Structured Sparse Problems at Scale: Continuous and Mixed Integer Programming Perspectives</h4>

<p>Structured sparsity plays an important role in high dimensional statistics and machine learning. They are naturally cast as solutions to nonconvex optimization problems. A major focus in this area has been on convex relaxations and/or greedy algorithms. Mixed Integer Programming (MIP) presents a flexible and effective framework for modeling and computation of these problems (to optimality). Despite promising recent research in this area, there is a considerable gap between the problem-sizes that can be handled via efficient MIP solvers versus fast algorithms to solve the convex relaxations. Compared to first order methods in convex optimization used in sparse learning, current efficient MIP solvers (e.g., commercial solvers) are less transparent, do not effectively exploit (statistical) problem-structure and can be computationally expensive. Convex optimization methods for sparse learning may provide insights into solving the corresponding MIP problems at scale.</p>

<p>To this end, we will discuss our recent work on sparse learning (e.g., best subset selection, hierarchical sparsity, sparse PCA). Our framework allows us to obtain near-optimal solutions to the discrete sparse learning problems at scales much larger than current state-of-the-art commercial solvers.  This enables us to algorithmically understand statistical properties of high-dimensional sparse estimators. This sheds interesting insights into the behavior of sparse learning estimators (e.g., the curious behavior of best-subsets across different SNR regimes) &mdash; properties that seem to be less known due to computational limitations.</p>

<h4 id="a-name-udell-a-madeleine-udell-optimal-storage-sdp"><a name="udell"></a> Madeleine Udell: Optimal storage SDP</h4>

<p>This talk develops new storage-optimal algorithms that provably
solve generic semidefinite programs (SDPs) in standard form.
The methods are particularly effective for weakly constrained SDPs.</p>

<p>The key idea is to formulate an approximate complementarity principle:
Given an approximate solution to the dual SDP,
the primal SDP has an approximate solution whose range is
contained in the null space of the dual slack matrix.
For weakly constrained SDPs, this null space has very low dimension,
so this observation significantly reduces the search space for the primal solution.</p>

<p>This result suggests an algorithmic strategy that can be implemented with minimal storage:
(1) Solve the dual SDP approximately;
(2) compress the primal SDP to the null space of the dual slack matrix;
(3) solve the compressed primal SDP.</p>

<h4 id="a-name-dustin-a-dustin-mixon-squeezefit-label-aware-dimensionality-reduction-via-semidefinite-programming"><a name="dustin"></a> Dustin Mixon:SqueezeFit: Label aware dimensionality reduction via semidefinite programming</h4>

<p>Given labeled points in a high-dimensional vector space, we seek a low-dimensional subspace such that projecting onto this subspace maintains some prescribed distance between points of differing labels. Intended applications include compressive classification. This talk will introduce a semidefinite relaxation of this problem, along with various performance guarantees. (Joint work with Culver McWhirter (OSU) and Soledad Villar (NYU).)</p>

<h4 id="a-name-facundo-a-facundo-memoli-gromov-wasserstein-distances-and-distributional-invariants-of-datasets"><a name="facundo"></a> Facundo Memoli: Gromov-Wasserstein distances and distributional invariants of datasets</h4>

<p>The Gromov-Wasserstein (GW) distance is a generalization of the standard Wasserstein distance between two probability measures on a given ambient metric space. The GW distance assumes that these two probability measures might live on different ambient spaces and therefore implements an actual comparison of pairs of metric measure spaces. Metric-measure spaces are triples (X,dX,muX) where (X,dX) is a metric space and muX is a Borel probability measure over X and serve as a model for datasets.</p>

<p>In practical applications, this distance is estimated either directly via gradient based optimization approaches, or through the computation of lower bounds which arise from distributional invariants of metric-measure spaces. One particular such invariant is the so called &lsquo;global distance distribution&rsquo; of pairwise distances.</p>

<p>This talk will overview the construction of the GW distance, the stability of distribution based invariants, and will discuss some results regarding the injectivity of the global distribution of distances for smooth planar curves.</p>

</div>


    </main>

    
  </body>
</html>
