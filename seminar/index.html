<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.96.0" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>the MAD Seminar &middot; Math and Data</title>

  
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="https://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="https://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
      
         
                <li>
                    <a href="/people/corefaculty/">
                        
                        <span>Faculty</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/phds/">
                        
                        <span>PhD/MsC Students</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/postdocs/">
                        
                        <span>Postdocs and Fellows</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/reading/">
                        
                        <span>Reading Groups</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/thanks/">
                        
                        <span>Sponsors</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/seminar/">
                        
                        <span>the MAD Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/madplus/">
                        
                        <span>the MAD&#43; Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/micsem/">
                        
                        <span>the MIC Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/miscpeople/">
                        
                        <span>Visitors and Alumni</span>
                    </a>
                </li>
        
      
    </ul>

  </div>
</div>



    <main class="content container">
    <div class="post">
  <h1>the MAD Seminar</h1>
  <p>The MaD seminar features leading specialists at the interface
of Applied Mathematics, Statistics and Machine Learning. It is partly supported by the Moore-Sloan Data Science Environment at NYU.</p>
<p><strong>We have resumed in-person MaD seminars.</strong> The seminars are also recorded and streamed live. Links to the videos are available below.</p>
<p><strong>Room:</strong> Auditorium Hall 150, Center for Data Science, NYU, <a href="https://www.google.com/maps/place/NYU+Center+for+Data+Science/@40.735016,-73.9969907,17z/data=!3m1!4b1!4m5!3m4!1s0x89c2599787834ad9:0x5dd8af15d9fbc8a3!8m2!3d40.735016!4d-73.994802">60 5th ave</a>.</p>
<p><strong>Time:</strong> 2:00pm-3:00pm</p>
<p><strong>Subscribe to the Seminar Mailing list <a href="http://cims.nyu.edu/mailman/listinfo/mad">here</a></strong></p>
<h3 id="schedule-with-confirmed-speakers">Schedule with Confirmed Speakers</h3>
<table>
<thead>
<tr>
<th>Date</th>
<th style="text-align:center">Speaker</th>
<th style="text-align:center">Title</th>
<th style="text-align:center">Live Stream</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mar 31</td>
<td style="text-align:center"><a href="https://www.wisdom.weizmann.ac.il/~ronene/">Ronen Eldan (Weizmann Institute of Science)</a></td>
<td style="text-align:center"><a href="#eldan">Localization schemes: A framework for the analysis of sampling algorithms</a></td>
<td style="text-align:center"><a href="https://nyu.zoom.us/j/94332538693">zoom link</a></td>
</tr>
<tr>
<td>Apr 14</td>
<td style="text-align:center"><a href="http://sbubeck.com/">Sébastien Bubeck (MSR)</a></td>
<td style="text-align:center"><a href="#bubeck">Set Chasing, with an application to online shortest path</a></td>
<td style="text-align:center"><a href="https://nyu.zoom.us/j/92524207563">zoom link</a></td>
</tr>
<tr>
<td>Apr 21</td>
<td style="text-align:center"><a href="https://rinafb.github.io/">Rina Foygel Barber (U Chicago)</a></td>
<td style="text-align:center"><a href="#barber">Conformal prediction beyond exchangeability</a></td>
<td style="text-align:center"><a href="https://cimsnyu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=e24c08f7-17e8-45a9-bb0a-ae76014386d0">link</a></td>
</tr>
<tr>
<td>April 28 <strong>Canceled</strong></td>
<td style="text-align:center"><del><a href="https://annacgilbert.github.io/">Anna Gilbert (Yale)</a></del></td>
<td style="text-align:center"><del><a href="#gilbert">Metric representations: Algorithms and Geometry</a></del></td>
<td></td>
</tr>
<tr>
<td>May 5</td>
<td style="text-align:center"><a href="https://math.mit.edu/~nsun/">Nike Sun (MIT)</a></td>
<td style="text-align:center"><a href="#sun">On the Ising perceptron</a></td>
<td style="text-align:center"><a href="https://nyu.zoom.us/j/92830550229">zoom link</a></td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2021/">Schedule Fall 2021</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2020/">Schedule Spring 2020</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2019/">Schedule Fall 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2019/">Schedule Spring 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2018/">Schedule Fall 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2018/">Schedule Spring 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2017/">Schedule Fall 2017</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2017/">Schedule Spring 2017</a></p>
<h3 id="abstracts">Abstracts</h3>
<h4 id="a-nameeldana-ronen-eldan-localization-schemes-a-framework-for-the-analysis-of-sampling-algorithms"><a name='eldan'></a> Ronen Eldan: Localization schemes: A framework for the analysis of sampling algorithms</h4>
<p>Two recent and seemingly-unrelated techniques for proving mixing bounds for Markov chains are: (i) the framework of “Spectral Independence”, introduced by Anari, Liu and Oveis Gharan, and its numerous extensions, which have given rise to several breakthroughs in the analysis of mixing times of discrete Markov chains and (ii) the Stochastic Localization technique which has proven useful in establishing mixing and expansion bounds for both log-concave measures and for measures on the discrete hypercube. In this talk, I’ll present a framework which aims to both unify and extend those techniques, thus providing an approach that gives bounds for sampling algorithms in both discrete and continuous settings. In its center is the concept of a &ldquo;localization scheme&rdquo; which, to every probability measure on some space $\Omega$ (which will usually be either the discrete hypercube or R^n), assigns a martingale of probability measures which &ldquo;localize&rdquo; in space as time evolves. As it turns out, every such scheme can be associated with a Markov chain, and many chains of interest (such as Glauber dynamics) appear naturally in this framework. This viewpoint provides tools for deriving mixing bounds for the dynamics through the analysis of the corresponding localization process. Generalizations of the concept of Spectral Independence naturally arise from our definitions, and in particular we will show how to recover the main theorems in the spectral independence framework via simple martingale arguments (completely bypassing the need to use the theory of high-dimensional expanders). We demonstrate how to apply our machinery towards simple proofs to many mixing bounds in the recent literature. We will briefly discuss some applications, among which are obtaining the first $O(n \log n)$ bound for mixing time of the hardcore-model (of arbitrary degree) in the tree-uniqueness regime, under Glauber dynamics and to proving a KL-divergence decay bound for log-concave sampling via the Restricted Gaussian Oracle, which achieves optimal mixing under any $\exp(n)$-warm start.</p>
<p>Based on a joint work with Yuansi Chen.</p>
<h4 id="a-namebubecka-sébastien-bubeck-set-chasing-with-an-application-to-online-shortest-path"><a name='bubeck'></a> Sébastien Bubeck: Set Chasing, with an application to online shortest path</h4>
<p>Since the late 19th century, mathematicians have realized the importance and generality of selection problems: given a collection of sets, select an element in each set, possibly in a ``nice” way. Of particular importance in computer science is the scenario where the ground set is a metric space, in which case it is natural to ask for <em>Lipschitz</em> selection (with Hausdorff distance between sets). In this talk I will describe a far-reaching extension of this classical Lipschitz selection problem to an <em>online</em> setting, where sets are streaming to the selector. I will show how Riemannian gradient descent (aka mirror descent) can be used to approach this type of problems. I will illustrate the power of the framework by solving a long-standing problem in online shortest path known as layered graph traversal (introduced by Papadimitriou and Yannakakis in 1989).</p>
<h4 id="a-namebarbera-rina-foygel-barber--conformal-prediction-beyond-exchangeability"><a name='barber'></a> Rina Foygel Barber:  Conformal prediction beyond exchangeability</h4>
<p>Conformal prediction is a popular, modern technique for providing valid predictive inference for arbitrary machine learning models. Its validity relies on the assumptions of exchangeability of the data, and symmetry of the given model fitting algorithm as a function of the data. However, exchangeability is often violated when predictive models are deployed in practice. For example, if the data distribution drifts over time, then the data points are no longer exchangeable; moreover, in such settings, we might want to use an algorithm that treats recent observations as more relevant, which would violate the assumption that data points are treated symmetrically. This paper proposes new methodology to deal with both aspects: we use weighted quantiles to introduce robustness against distribution drift, and design a new technique to allow for algorithms that do not treat data points symmetrically, with theoretical results verifying coverage guarantees that are robust to violations of exchangeability.</p>
<p>This work is joint with Emmanuel Candes, Aaditya Ramdas, and Ryan Tibshirani.</p>
<h4 id="a-namegilberta-anna-gilbert--metric-representations-algorithms-and-geometry"><a name='gilbert'></a> Anna Gilbert:  Metric representations: Algorithms and Geometry</h4>
<p>Given a set of distances amongst points, determining what metric representation is most “consistent” with the input distances or the metric that best captures the relevant geometric features of the data is a key step in many machine learning algorithms. In this talk, we discuss a number of variants of this problem, from convex optimization problems with metric constraints to sparse metric repair.</p>
<h4 id="a-namesuna-nike-sun--on-the-ising-perceptron"><a name='sun'></a> Nike Sun:  On the Ising perceptron</h4>
<p>The perceptron is a toy model of a single-layer neural network that &ldquo;stores&rdquo; a collection of given patterns. The model with N nodes and M=N*alpha random patterns is related to a natural problem in high-dimensional probability, concerning the intersection of M random half-spaces with the discrete cube or sphere in N dimensions. We also consider a more general version of the problem, defined by bounded activation functions, which was introduced and studied by Talagrand. We will present some new techniques and recent results on these models.
Based on joint works with Jian Ding, Erwin Bolthausen, Shuta Nakajima, and Changji Xu.</p>

</div>


    </main>

    
  </body>
</html>
