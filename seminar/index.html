<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.119.0">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>the MAD Seminar &middot; Math and Data</title>

  
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="https://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="https://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
      
         
                <li>
                    <a href="/people/corefaculty/">
                        
                        <span>Faculty</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/phds/">
                        
                        <span>PhD/MsC Students</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/postdocs/">
                        
                        <span>Postdocs and Fellows</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/reading/">
                        
                        <span>Reading Groups</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/thanks/">
                        
                        <span>Sponsors</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/seminar/">
                        
                        <span>the MAD Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/madplus/">
                        
                        <span>the MAD&#43; Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/micsem/">
                        
                        <span>the MIC Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/miscpeople/">
                        
                        <span>Visitors and Alumni</span>
                    </a>
                </li>
        
      
    </ul>

  </div>
</div>



    <main class="content container">
    <div class="post">
  <h1>the MAD Seminar</h1>
  <p>The MaD seminar features leading specialists at the interface
of Applied Mathematics, Statistics and Machine Learning.</p>
<p><strong>Room:</strong> Auditorium Hall 150, Center for Data Science, NYU, <a href="https://www.google.com/maps/place/NYU+Center+for+Data+Science/@40.735016,-73.9969907,17z/data=!3m1!4b1!4m5!3m4!1s0x89c2599787834ad9:0x5dd8af15d9fbc8a3!8m2!3d40.735016!4d-73.994802">60 5th ave</a>.</p>
<p><strong>Time:</strong> 2:00pm-3:00pm</p>
<p><strong>Subscribe to the Seminar Mailing list <a href="http://cims.nyu.edu/mailman/listinfo/mad">here</a></strong></p>
<h3 id="schedule-with-confirmed-speakers">Schedule with Confirmed Speakers</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Date</th>
<th style="text-align:center">Speaker</th>
<th style="text-align:center">Title</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">September 21</td>
<td style="text-align:center"><a href="https://yanjunhan2021.github.io/">Yanjun Han (NYU)</a></td>
<td style="text-align:center"><a href="#han">Two recent lower bounds for interactive decision making</a></td>
</tr>
<tr>
<td style="text-align:center">October 5</td>
<td style="text-align:center"><a href="https://sma.epfl.ch/~abbe/">Emmanuel Abbe (EPFL)</a></td>
<td style="text-align:center"><a href="#abbe">Logic reasoning and generalization on the unseen</a></td>
</tr>
<tr>
<td style="text-align:center">October 12</td>
<td style="text-align:center"><a href="https://yutingwei.github.io">Yuting Wei (UPenn)</a></td>
<td style="text-align:center"><a href="#wei">Approximate message passing: A non-asymptotic framework and beyond</a></td>
</tr>
<tr>
<td style="text-align:center">October 19</td>
<td style="text-align:center"><a href="https://florentkrzakala.com">Florent Krzakala (EPFL)</a></td>
<td style="text-align:center"><a href="#krzakala">How Two-Layer Neural Networks Learn Functions, One (Giant) Step at a Time</a></td>
</tr>
<tr>
<td style="text-align:center">October 26</td>
<td style="text-align:center"><a href="https://math.mit.edu/~rigollet/">Philippe Rigollet (MIT)</a></td>
<td style="text-align:center"><a href="#rigollet">A mathematical perspective on transformers</a></td>
</tr>
<tr>
<td style="text-align:center">November 2</td>
<td style="text-align:center"><a href="http://stat.wharton.upenn.edu/~suw/">Weijie Su (UPenn)</a></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">November 9</td>
<td style="text-align:center"><a href="https://qingqu.engin.umich.edu">Qing Qu (UMich)</a></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">November 16</td>
<td style="text-align:center"><a href="https://danayang.github.io">Dana Yang (Cornell)</a></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">December 7</td>
<td style="text-align:center"><a href="https://sites.google.com/site/malekiarian/">Arian Maleki (Columbia)</a></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h3 id="abstracts">Abstracts</h3>
<h4 id="a-namehan-yanjun-han-two-recent-lower-bounds-for-interactive-decision-makinga"><a name="han"> Yanjun Han: Two recent lower bounds for interactive decision making</a></h4>
<p>A fundamental problem in interactive decision making, ranging from bandit problems to reinforcement learning, is to understand what modeling assumptions lead to sample-efficient learning guarantees, and what algorithm design principles achieve optimal sample complexity. While both questions are well understood for classical problems of statistical estimation and learning, there are relatively fewer tools to analyze the fundamental limits for the interactive counterparts.</p>
<p>In this talk I will present two general lower bound techniques for interactive decision making. First, we introduce a complexity measure, called the Constrained Decision-Estimation Coefficient, which is an interactive counterpart of the Donoho-Liu type modulus of continuity in statistical estimation. This complexity measure provides a lower bound of the optimal regret for general interactive problems, as well as a matching upper bound up to an online estimation error. Second, we attempt to close this gap via a generalization of the Fano type arguments, using a suitable notion of information for interactive problems. In a special class of problems called ridge bandits, our new tool leads to lower bounds on the entire learning trajectory via differential equations. We also provide upper bounds that evolve with similar differential equations, and thereby showcase the complication of finding a unified complexity measure in general.</p>
<p>Based on recent work <a href="https://arxiv.org/abs/2301.08215">https://arxiv.org/abs/2301.08215</a> and <a href="https://arxiv.org/abs/2302.06025">https://arxiv.org/abs/2302.06025</a>, jointly with Dylan Foster, Noah Golowich, Jiantao Jiao, Nived Rajaraman, and Kannan Ramchandran.</p>
<h4 id="a-nameabbe-emmanuel-abbe-logic-reasoning-and-generalization-on-the-unseena"><a name="abbe"> Emmanuel Abbe: Logic reasoning and generalization on the unseen</a></h4>
<p>Transformers have become the dominant neural network architecture in deep learning. While they are state of the art in language and vision tasks, their performance is less convincing in so-called “reasoning” tasks. In this talk, we consider the “generalization on the unseen (GOTU)” objective to test the reasoning capabilities of neural networks, primarily Transformers on Boolean/logic tasks. We first give experimental results showing that such networks have a strong “minimal degree bias”: they tend to find specific interpolators having low degree, in agreement with the “leap complexity” picture derived for classic generalization. Using basic concepts from Boolean Fourier analysis and algebraic geometry, we then characterize such minimal degree profile interpolators and prove two theorems about the convergence of (S)GD to such interpolators on basic architectures. Since the minimal degree profile is not desirable in many reasoning tasks, we discuss methods to correct this bias and improve consequently the reasoning capabilities. Based on joint works with S. Bengio, A. Lotfi, K. Rizk and E. Adsera-Boix, T. Misiakiewicz.</p>
<h4 id="a-namewei-yuting-wei-approximate-message-passing-a-non-asymptotic-framework-and-beyond-a"><a name="wei"> Yuting Wei: Approximate message passing: A non-asymptotic framework and beyond </a></h4>
<p>Approximate message passing (AMP) emerges as an effective iterative algorithm for solving high-dimensional statistical problems. However, prior AMP theory, which focused mostly on high-dimensional asymptotics, fell short of predicting the AMP dynamics when the number of iterations surpasses o(log n / log log n) (with n the problem dimension). To address this inadequacy, this talk introduces a non-asymptotic framework towards understanding AMP. Built upon a new decomposition of AMP updates in conjunction with well-controlled residual terms, we lay out an analysis recipe to characterize the finite-sample convergence of AMP up to O(n / polylog(n)) iterations. We will discuss concrete consequences of the proposed analysis recipe in the Z2 synchronization problem; more specifically, we predict the behavior of randomly initialized AMP for up to O(n/poly(\log n)) iterations, showing that the algorithm succeeds without the need of a careful spectral initialization and also a subsequent refinement stage (as conjectured recently by Celentano et al.)</p>
<h4 id="a-namekrzakala-florent-krzakala-how-two-layer-neural-networks-learn-functions-one-giant-step-at-a-time-a"><a name="krzakala"> Florent Krzakala: How Two-Layer Neural Networks Learn Functions, One (Giant) Step at a Time </a></h4>
<p>How do two-layer neural networks learn complex functions from data over time? In this talk, I will delve into the interaction between batch size, number of iterations, and task complexity, shedding light on neural network adaptation to data features. I will particularly discuss three key findings: i) The significant impact of a single gradient step on feature learning, emphasizing the relationship between batch size and the target&rsquo;s information exponent (or complexity). ii) The enhancement of the network&rsquo;s approximation ability over multiple gradient steps, enabling the learning of more intricate functions over time. iii) The improvement in generalization compared to the basic random feature/kernel regime. Our theoretical approach combines techniques from statistical physics, concentration of measure, projection-based conditioning, and Gaussian equivalence, which we believe hold standalone significance. By identifying the prerequisites for specialization and learning, our results offer a comprehensive mathematical theory on two-layer neural network data representation learning during training and its influence on generalization.</p>
<h4 id="a-namerigollet-philippe-rigollet-a-mathematical-perspective-on-transformers-a"><a name="rigollet"> Philippe Rigollet: A mathematical perspective on transformers </a></h4>
<p>In just five years since their introduction, Transformers have revolutionized large language models and the broader field of deep learning. Central to this transformative success is the groundbreaking self-attention mechanism. In this presentation, I&rsquo;ll introduce a mathematical framework that casts this mechanism as a mean-field interacting particle system, revealing a desirable long-time clustering behavior. This perspective leads to a trove of fascinating questions with unexpected connections to Kuramoto oscillators, sphere packing, and Wasserstein gradient flows.</p>
<hr>
<h3 id="archive">Archive</h3>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2023/">Schedule Spring 2023</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2022/">Schedule Fall 2022</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2022/">Schedule Spring 2022</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2021/">Schedule Fall 2021</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2020/">Schedule Spring 2020</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2019/">Schedule Fall 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2019/">Schedule Spring 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2018/">Schedule Fall 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2018/">Schedule Spring 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2017/">Schedule Fall 2017</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2017/">Schedule Spring 2017</a></p>

</div>


    </main>

    
  </body>
</html>
