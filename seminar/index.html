<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.68.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>the MAD Seminar &middot; Math and Data</title>

  
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="https://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="https://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
      
         
                <li>
                    <a href="/people/corefaculty/">
                        
                        <span>Faculty</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/phds/">
                        
                        <span>PhD/MsC Students</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/postdocs/">
                        
                        <span>Postdocs and Fellows</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/reading/">
                        
                        <span>Reading Groups</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/thanks/">
                        
                        <span>Sponsors</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/seminar/">
                        
                        <span>the MAD Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/madplus/">
                        
                        <span>the MAD&#43; Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/micsem/">
                        
                        <span>the MIC Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/miscpeople/">
                        
                        <span>Visitors and Alumni</span>
                    </a>
                </li>
        
      
    </ul>

  </div>
</div>



    <main class="content container">
    <div class="post">
  <h1>the MAD Seminar</h1>
  <p>The MaD seminar features leading specialists at the interface
of Applied Mathematics, Statistics and Machine Learning. It is partly supported by the Moore-Sloan Data Science Environment at NYU.</p>
<p><strong>Room:</strong> Auditorium Hall 150, Center for Data Science, NYU, <a href="https://www.google.com/maps/place/NYU+Center+for+Data+Science/@40.735016,-73.9969907,17z/data=!3m1!4b1!4m5!3m4!1s0x89c2599787834ad9:0x5dd8af15d9fbc8a3!8m2!3d40.735016!4d-73.994802">60 5th ave</a>.</p>
<p><strong>Time:</strong> 2:00pm-3:00pm</p>
<p><strong>Subscribe to the Seminar Mailing list <a href="http://cims.nyu.edu/mailman/listinfo/mad">here</a></strong></p>
<h3 id="schedule-with-confirmed-speakers">Schedule with Confirmed Speakers</h3>
<table>
<thead>
<tr>
<th align="center">Date</th>
<th align="center">Speaker</th>
<th align="center">Title</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center">February 9</td>
<td align="center"><a href="http://www.columbia.edu/~cgr2130/">Cynthia Rush (Columbia)</a></td>
<td align="center"><a href="#rush">Exact Asymptotics with Approximate Message Passing and a Study of the Type 1-Type 2 Error Trade-off for SLOPE</a></td>
</tr>
<tr>
<td align="center">February 16</td>
<td align="center"><a href="https://sdean.website/">Sarah Dean (Cornell)</a></td>
<td align="center"><a href="#sarah">Participation Dynamics in Learning Systems</a></td>
</tr>
<tr>
<td align="center">February 23</td>
<td align="center"><a href="https://menard.pha.jhu.edu/">Brice MÃ©nard (JHU)</a></td>
<td align="center"><a href="#brice">Opening the neural network black box</a></td>
</tr>
<tr>
<td align="center">March 16</td>
<td align="center"><a href="https://maxim.ece.illinois.edu/">Maxim Raginsky (UIUC)</a></td>
<td align="center"><a href="#maxim">Variational Principles for Mirror Descent and Mirror Langevin Dynamics</a></td>
</tr>
<tr>
<td align="center">March 23</td>
<td align="center">Jose Blanchet</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">March 30</td>
<td align="center">Jason Lee</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">April 6</td>
<td align="center">Surbhi Goel</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">April 13</td>
<td align="center">Lenka Zdeborova</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">April 20</td>
<td align="center">Andre Wibisono</td>
<td align="center"></td>
</tr>
<tr>
<td align="center">April 27</td>
<td align="center">Jean Ponce</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2022/">Schedule Fall 2022</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2022/">Schedule Spring 2022</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2021/">Schedule Fall 2021</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2020/">Schedule Spring 2020</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2019/">Schedule Fall 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2019/">Schedule Spring 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2018/">Schedule Fall 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2018/">Schedule Spring 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2017/">Schedule Fall 2017</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2017/">Schedule Spring 2017</a></p>
<h3 id="abstracts">Abstracts</h3>
<h4 id="a-namerusha-cynthia-rush-exact-asymptotics-with-approximate-message-passing-and-a-study-of-the-type-1-type-2-error-trade-off-for-slope"><a name='rush'></a> Cynthia Rush: Exact Asymptotics with Approximate Message Passing and a Study of the Type 1-Type 2 Error Trade-off for SLOPE</h4>
<p>Approximate message passing (AMP) is a class of iterative algorithms that can be used to systematically derive exact expressions for the asymptotic risk and other performance metrics for estimators that are constructed as solutions to a broad class of convex optimization problems. In this talk, we present a general program for using AMP in this way and we provide a specific example by using this approach to study the asymptotic model selection properties of sorted L1 penalized estimation (SLOPE). Sorted L1 regularization has been incorporated into many methods for solving high-dimensional statistical estimation problems, including using SLOPE in the context of linear regression. We will show how this regularization technique improves variable selection relative to the LASSO by characterizing the optimal SLOPE trade-off between the false discovery proportion and true positive proportion or, equivalently, between measures of type I and type II error. Collaborators on this work include Zhiqi Bu, Jason Klusowski, and Weijie Su (<a href="https://arxiv.org/abs/1907.07502">https://arxiv.org/abs/1907.07502</a> and <a href="https://arxiv.org/abs/2105.13302">https://arxiv.org/abs/2105.13302</a>) and Oliver Feng, Ramji Venkataramanan, and Richard Samworth (<a href="https://arxiv.org/abs/2105.02180)">https://arxiv.org/abs/2105.02180)</a>.</p>
<h4 id="a-namesaraha-sarah-dean-participation-dynamics-in-learning-systems"><a name='sarah'></a> Sarah Dean: Participation Dynamics in Learning Systems</h4>
<p>The choice to participate in a data-driven system, often made on the basis of the quality of that system, influences the ability of the system to learn and improve. Participation choices manifest as distribution shifts which are partially endogeneous, i.e. caused by the machine learning system itself. In this talk, I will discuss participation dynamics in the presence of multiple learners. We introduce and study a general class of loss-reducing dynamics, in which learners retrain to improve predictive performance and users shift participation towards better performing learners. We characterize the stable equilibria and discuss the implications in terms of social welfare and fairness. Based on joint work with Mihaela Curmei, Maryam Fazel, Jamie Morgenstern, and Lillian Ratliff.</p>
<h4 id="a-namebricea-brice-menard-opening-the-neural-network-black-box"><a name='brice'></a> Brice Menard: Opening the neural network black box</h4>
<p>I will present a simple point of view allowing us to make sense of the weights in a trained neural network. I will show how to characterize what has been learned, extract quasi-sufficient summary statistics, and use them to generate new networks performing well without any training. I will show that the symmetry group of neural networks are layer-based rotations. When taken into account network weights always converge to the same solution. I will illustrate these results using standard classification tasks on CIFAR-10 and ImageNet and I will introduce a model that captures all these properties. Finally, I will show that most of the stochasticity inherent to neural networks and their training is largely negligible. Collaborators: F. Guth, S. Mallat &amp; G. Rochette.</p>
<h4 id="a-namemaxima-maxim-raginsky-variational-principles-for-mirror-descent-and-mirror-langevin-dynamics"><a name='maxim'></a> Maxim Raginsky: Variational Principles for Mirror Descent and Mirror Langevin Dynamics</h4>
<p>Mirror descent, introduced by Nemirovsky and Yudin in the 1970s, is a primal-dual convex optimization method that can be tailored to the geometry of the optimization problem at hand through the choice of a strongly convex distance-generating potential function. It arises as a basic primitive in a variety of applications, including large-scale optimization, machine learning, and control. In this talk, based on joint work with Belinda Tzen, Anant Raj, and Francis Bach, I will discuss a variational formulation of mirror descent and of its stochastic variant, mirror Langevin dynamics. The main idea, inspired by classic work of Brezis and Ekeland, is to show that mirror descent emerges as a closed-loop solution for a certain optimal control problem, and the Bellman value function is given by the dual-space Bregman divergence between the initial condition and the global minimizer of the objective function. This formulation has several interesting corollaries and implications, including a form of implicit regularization, which I will discuss.</p>

</div>


    </main>

    
  </body>
</html>
