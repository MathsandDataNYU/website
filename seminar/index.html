<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.88.1" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>the MAD Seminar &middot; Math and Data</title>

  
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="https://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="https://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
      
         
                <li>
                    <a href="/people/corefaculty/">
                        
                        <span>Faculty</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/phds/">
                        
                        <span>PhD/MsC Students</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/postdocs/">
                        
                        <span>Postdocs and Fellows</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/reading/">
                        
                        <span>Reading Groups</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/thanks/">
                        
                        <span>Sponsors</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/seminar/">
                        
                        <span>the MAD Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/madplus/">
                        
                        <span>the MAD&#43; Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/micsem/">
                        
                        <span>the MIC Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/miscpeople/">
                        
                        <span>Visitors and Alumni</span>
                    </a>
                </li>
        
      
    </ul>

  </div>
</div>



    <main class="content container">
    <div class="post">
  <h1>the MAD Seminar</h1>
  <p>The MaD seminar features leading specialists at the interface
of Applied Mathematics, Statistics and Machine Learning. It is partly supported by the Moore-Sloan Data Science Environment at NYU.</p>
<p><strong>We have resumed in-person MaD seminars.</strong> The seminars are also recorded and streamed live. Links to the videos are available below.</p>
<p><strong>Room:</strong> Auditorium Hall 150, Center for Data Science, NYU, <a href="https://www.google.com/maps/place/NYU+Center+for+Data+Science/@40.735016,-73.9969907,17z/data=!3m1!4b1!4m5!3m4!1s0x89c2599787834ad9:0x5dd8af15d9fbc8a3!8m2!3d40.735016!4d-73.994802">60 5th ave</a>.</p>
<p><strong>Time:</strong> 2:00pm-3:00pm</p>
<p><strong>Subscribe to the Seminar Mailing list <a href="http://cims.nyu.edu/mailman/listinfo/mad">here</a></strong></p>
<h3 id="schedule-with-confirmed-speakers">Schedule with Confirmed Speakers</h3>
<table>
<thead>
<tr>
<th>Date</th>
<th style="text-align:center">Speaker</th>
<th style="text-align:center">Title</th>
<th style="text-align:center">Live Stream</th>
</tr>
</thead>
<tbody>
<tr>
<td>Oct 14</td>
<td style="text-align:center">Florentina Bunea (Cornell)</td>
<td style="text-align:center"><a href="#bunea">Surprises in topic model estimation and  new Wasserstein document-distance calculations</a></td>
<td style="text-align:center"><a href="https://nyu.zoom.us/rec/share/bUkMQLzr9q9Udm60GwhTJWukZ_8eAODKa54TSxiijukED7JatUQbAWs2XDI4X-JV.46u4Wg0wisfHD0mD?startTime=1634234470000">recording</a></td>
</tr>
<tr>
<td>Oct 21</td>
<td style="text-align:center">Tim Roughgarden (Columbia)</td>
<td style="text-align:center"><a href="#roughgarden">Smoothed Analysis of Online Learning</a></td>
<td style="text-align:center"><a href="https://nyu.zoom.us/j/91226308483">zoom</a></td>
</tr>
<tr>
<td>Oct 28</td>
<td style="text-align:center">Gemma Moran (Columbia)</td>
<td style="text-align:center"><a href="#moran">Identifiable Variational Autoencoders via Sparse Decoding</a></td>
<td style="text-align:center"><a href="https://nyu.zoom.us/j/93232052392">zoom</a></td>
</tr>
<tr>
<td>Nov 4</td>
<td style="text-align:center">Yanjun Han (Simons Institute)</td>
<td style="text-align:center"></td>
<td></td>
</tr>
<tr>
<td>Nov 11</td>
<td style="text-align:center">Alnur Ali (Stanford)</td>
<td style="text-align:center"></td>
<td></td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2020/">Schedule Spring 2020</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2019/">Schedule Fall 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2019/">Schedule Spring 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2018/">Schedule Fall 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2018/">Schedule Spring 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2017/">Schedule Fall 2017</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2017/">Schedule Spring 2017</a></p>
<h3 id="abstracts">Abstracts</h3>
<h4 id="a-namebuneaa-florentina-bunea-surprises-in-topic-model-estimation-and--new-wasserstein-document-distance-calculations"><a name='bunea'></a> Florentina Bunea: Surprises in topic model estimation and  new Wasserstein document-distance calculations</h4>
<p>Topic models have been and continue to be an important modeling tool for an ensemble of
independent multinomial samples with shared commonality. Although applications of topic
models span many disciplines, the jargon used to define them stems from text analysis.
In keeping with the standard terminology, one has access to a corpus of n independent
documents, each utilizing words from a given dictionary of size p. One draws N words
from each document and records their respective count, thereby representing the corpus as
a collection of n samples from independent, p-dimensional, multinomial distributions, each
having a different, document specific, true word probability vector Π. The topic model
assumption is that each Π is a mixture of K discrete distributions, that are common to
the corpus, with document specific mixture weights. The corpus is assumed to cover K
topics, that are not directly observable, and each of the K mixture components correspond
to conditional probabilities of words, given a topic. The vector of the K mixture weights,
per document, is viewed as a document specific topic distribution T, and is thus expected
to be sparse, as most documents will only cover a few of the K topics of the corpus.</p>
<p>Despite the large body of work on learning topic models, the estimation of sparse topic
distributions, of unknown sparsity, especially when the mixture components are not known,
and are estimated from the same corpus, is not well understood and will be the focus of
this talk. We provide estimators of T, with sharp theoretical guarantees, valid in many
practically relevant situations, including the scenario p &raquo; N (short documents, sparse
data) and unknown K. Moreover, the results are valid when dimensions p and K are
allowed to grow with the sample sizes N and n.</p>
<p>When the mixture components are known, we propose MLE estimation of the sparse
vector T, the analysis of which has been open until now. The surprising result, and a
remarkable property of the MLE in these models, is that, under appropriate conditions, and
without further regularization, it can be exactly sparse, and contain the true zero pattern
of the target. When the mixture components are not known, we exhibit computationally
fast and rate optimal estimators for them, and propose a quasi-MLE estimator of T, shown
to retain the properties of the MLE. The practical implication of our sharp, finite-sample,
rate analyses of the MLE and quasi-MLE reveal that having short documents can be
compensated for, in terms of estimation precision, by having a large corpus.</p>
<p>Our main application is to the estimation of 1-Wasserstein distances between document
generating distributions. We propose, estimate and analyze new 1-Wasserstein distances
between alternative probabilistic document representations, at the word and topic level,
respectively. The effectiveness of the proposed 1-Wasserstein distances, and contrast with
the more commonly used WMD between empirical frequency estimates, is illustrated by
an analysis of an IMDB movie reviews data set.</p>
<h4 id="a-nameroughgardena-tim-roughgarden-smoothed-analysis-of-online-learning"><a name='roughgarden'></a> Tim Roughgarden: Smoothed Analysis of Online Learning</h4>
<p>We consider a smoothed model of online learning: at each time step the learning algorithm chooses a hypothesis h from a class H, an adversary then selects a point of the domain, and finally nature slightly perturbs the adversary&rsquo;s choice.  The goal of the learning algorithm is to minimize regret with respect to the best hypothesis of H in hindsight.  Without perturbations (i.e., with a worst-case adversary), the feasibility of achieving vanishing regret is controlled by the finiteness of the Littlestone dimension of H.  Unfortunately, even one-dimensional threshold functions have infinite Littlestone dimension.  We show that in our smoothed model, good regret guarantees are instead controlled by the VC dimension of the hypothesis class, a parameter historically associated with batch (non-online) learning that is generally much smaller than the Littlestone dimensions (e.g., the VC dimension of bounded-degree multivariate polynomial threshold functions is finite).  Our main technical tool is a novel coupling that, in effect, reduces the setting of a smoothed adaptive adversary to the much simpler setting of an oblivious adversary (with all data points distributions chosen in advance).</p>
<p>Joint work with Nika Haghtalab and Abhishek Shetty, based on work appearing in NeurIPS &lsquo;20 and FOCS &lsquo;21.</p>
<h4 id="a-namemorana-gemma-moran-identifiable-variational-autoencoders-via-sparse-decoding"><a name='moran'></a> Gemma Moran: Identifiable Variational Autoencoders via Sparse Decoding</h4>
<p>We develop the Sparse VAE, a deep generative model for unsupervised representation learning on high-dimensional data.  Given a dataset of observations, the Sparse VAE learns a set of latent factors that captures its distribution.  The model is sparse in the sense that each feature of the dataset (i.e., each dimension) depends on a small subset of the latent factors.  As examples, in ratings data each movie is only described by a few genres; in text data each word is only applicable to a few topics; in genomics, each gene is active in only a few biological processes.  We first show that the Sparse VAE is identifiable: given data drawn from the model, there exists a uniquely optimal set of factors.  (In contrast, most VAE-based models are not identifiable.)  The key assumption behind Sparse-VAE identifiability is the existence of ``anchor features&quot;, where for each factor there exists a feature that depends only on that factor. Importantly, the anchor features do not need to be known in advance. We then show how to fit the Sparse VAE with variational EM.  Finally, we empirically study the Sparse VAE with both simulated and real data.  We find that it recovers meaningful latent factors and has smaller heldout reconstruction error than related methods.</p>

</div>


    </main>

    
  </body>
</html>
