<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.26" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
  <title>the MaD Seminar &middot; Math and Data</title>
  

  
  <link rel="stylesheet" href="http://mad.cds.nyu.edu/css/poole.css">
  <link rel="stylesheet" href="http://mad.cds.nyu.edu/css/syntax.css">
  <link rel="stylesheet" href="http://mad.cds.nyu.edu/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Math and Data" />
</head>

	
<body class=" ">
	
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="http://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="http://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
        <li><a href="/about/"> About </a></li>
      
        <li><a href="/people/"> People </a></li>
      
        <li><a href="/reading/"> Reading Groups </a></li>
      
        <li><a href="/micsem/"> the MIC Seminar </a></li>
      
        <li><a href="/seminar/"> the MaD Seminar </a></li>
      
    </ul>

    <p>&copy; 2019. All rights reserved. </p>
  </div>
</div>


		<div class="content container">
			<div class="post">
			 	<h1>the MaD Seminar</h1>
		
			      

<p>The MaD seminar features leading specialists at the interface
of Applied Mathematics, Statistics and Machine Learning. It is partly supported by the Moore-Sloan Data Science Environment at NYU.</p>

<p><strong>Room:</strong> Auditorium Hall 150, Center for Data Science, NYU, <a href="https://www.google.com/maps/place/NYU+Center+for+Data+Science/@40.735016,-73.9969907,17z/data=!3m1!4b1!4m5!3m4!1s0x89c2599787834ad9:0x5dd8af15d9fbc8a3!8m2!3d40.735016!4d-73.994802">60 5th ave</a>.</p>

<p><strong>Time:</strong> 2:00pm-3:00pm, Reception will follow.</p>

<p><strong>Subscribe to the Seminar Mailing list <a href="http://cims.nyu.edu/mailman/listinfo/mad">here</a></strong></p>

<h3 id="schedule-with-confirmed-speakers">Schedule with Confirmed Speakers</h3>

<table>
<thead>
<tr>
<th>Date</th>
<th align="center">Speaker</th>
<th align="center">Title</th>
</tr>
</thead>

<tbody>
<tr>
<td>Feb 21</td>
<td align="center"><a href="https://scholar.google.com/citations?user=cn_FoswAAAAJ&amp;hl=en">Jeffrey Pennington (Google)</a></td>
<td align="center"><a href="#jeffrey">Dynamical Isometry and a Mean Field Theory of Signal Propagation in Deep Neural Networks</a></td>
</tr>

<tr>
<td>Feb 28</td>
<td align="center">Ahmed El Alaoui (Stanford)</td>
<td align="center"></td>
</tr>

<tr>
<td>Mar 7</td>
<td align="center">Florent Krzakala (ENS)</td>
<td align="center"></td>
</tr>

<tr>
<td>Mar 14</td>
<td align="center">Rina Foygel Barber (Chicago)</td>
<td align="center"></td>
</tr>

<tr>
<td>Mar 21</td>
<td align="center">(Spring break)</td>
<td align="center"></td>
</tr>

<tr>
<td>Mar 28</td>
<td align="center">Victor Preciado (UPenn)</td>
<td align="center"></td>
</tr>

<tr>
<td>Apr 4</td>
<td align="center"></td>
<td align="center"></td>
</tr>

<tr>
<td>Apr 11</td>
<td align="center">Sam Hopkins (UC Berkeley)</td>
<td align="center"></td>
</tr>

<tr>
<td>Apr 18</td>
<td align="center"></td>
<td align="center"></td>
</tr>

<tr>
<td>Apr 25</td>
<td align="center"></td>
<td align="center"></td>
</tr>

<tr>
<td>May 2</td>
<td align="center">Rayan Saab (UC San Diego)</td>
<td align="center"></td>
</tr>

<tr>
<td>May 9</td>
<td align="center"></td>
<td align="center"></td>
</tr>
</tbody>
</table>

<hr />

<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2018/">Schedule Fall 2018</a></p>

<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2018/">Schedule Spring 2018</a></p>

<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2017/">Schedule Fall 2017</a></p>

<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2017/">Schedule Spring 2017</a></p>

<hr />

<h3 id="abstracts">Abstracts</h3>

<h4 id="a-name-jeffrey-a-jeffrey-pennington-dynamical-isometry-and-a-mean-field-theory-of-signal-propagation-in-deep-neural-networks"><a name="jeffrey"></a> Jeffrey Pennington: Dynamical Isometry and a Mean Field Theory of Signal Propagation in Deep Neural Networks</h4>

<p>In recent years, many state-of-the-art models in deep learning have utilized increasingly deep architectures, with some successful models like deep residual networks employing hundreds or even thousands of layers. In sequence modeling, recurrent neural networks are often trained over a similarly large number of time steps. Yet despite their widespread use, deep neural networks remain notoriously difficult to train. In this talk, I will develop a theory of signal propagation in deep networks and argue that training is feasible precisely when signals can propagate without attenuation or distortion. I will provide a theoretical characterization of these conditions, which amounts to an architecture-dependent initialization scheme. Using this type of initialization, I will show that it is possible to train vanilla convolutional networks with over 10,000 layers and that convergence rates for recurrent networks can be improved by orders of magnitude.</p>

			</div>

			
		</div>

  </body>
</html>
