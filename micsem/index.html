<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.18.1" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  
  <title>the MIC Seminar &middot; Math and Data</title>
  

  
  <link rel="stylesheet" href="http://mad.cds.nyu.edu/css/poole.css">
  <link rel="stylesheet" href="http://mad.cds.nyu.edu/css/syntax.css">
  <link rel="stylesheet" href="http://mad.cds.nyu.edu/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Math and Data" />
</head>

	
<body class=" ">
	
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="http://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="http://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
        <li><a href="/about/"> About </a></li>
      
        <li><a href="/people/"> People </a></li>
      
        <li><a href="/reading/"> Reading Groups </a></li>
      
        <li><a href="/micsem/"> the MIC Seminar </a></li>
      
        <li><a href="/seminar/"> the MaD Seminar </a></li>
      
        <li><a href="/seminar_spring2018/"> the MaD Seminar Spring 2018 </a></li>
      
    </ul>

    <p>&copy; 2019. All rights reserved. </p>
  </div>
</div>


		<div class="content container">
			<div class="post">
			 	<h1>the MIC Seminar</h1>
		
			      

<p>The Mathematics, Information and Computation (MIC) Seminar runs at irregular intervals and covers specific aspects at the interface of applied maths, information theory and theory of computation.</p>

<h3 id="schedule-fall-18">Schedule Fall 18</h3>

<p>During the Fall the MIC Seminar will usually be in room 312 Tuesdays 6-7p.</p>

<table>
<thead>
<tr>
<th>Date</th>
<th align="center">Speaker</th>
<th align="center">Title</th>
<th align="left">Room</th>
</tr>
</thead>

<tbody>
<tr>
<td>Oct 16, 6p</td>
<td align="center"><a href="https://web.njit.edu/~shirokof/">David Shirokoff</a> (NJIT)</td>
<td align="center"><a href="#david">Convex relaxations for variational problems arising from models of self-assembly</a></td>
<td align="left">WWH 312</td>
</tr>

<tr>
<td>Oct 23, 6p</td>
<td align="center"><a href="https://homepages.laas.fr/henrion/">Didier Henrion</a> (CNRS)</td>
<td align="center">Solving nonlinear PDEs with the Lasserre hierarchy</td>
<td align="left">WWH 312</td>
</tr>

<tr>
<td>Nov 2, 1:30p</td>
<td align="center"><a href="https://math.berkeley.edu/~seigal/">Anna Seigal</a> (Berkeley)</td>
<td align="center"><a href="#anna">Structured Tensors and the Geometry of Data</a></td>
<td align="left">WWH 202</td>
</tr>

<tr>
<td>Nov 12, 12:15p</td>
<td align="center">Marylou Gabrie (ENS)</td>
<td align="center"><a href="#marylou">Entropy and mutual information in models of deep neural networks</a></td>
<td align="left">CDS 650</td>
</tr>

<tr>
<td>Nov 13, 6:00p</td>
<td align="center">Aukosh Jagannath (Harvard)</td>
<td align="center"><a href="#aukosh">Algorithmic thresholds for tensor principle component analysis</a></td>
<td align="left">WWH 312</td>
</tr>
</tbody>
</table>

<h3 id="abstracts">Abstracts</h3>

<h4 id="a-name-marylou-a-aukosh-jagannath-algorithmic-thresholds-for-tensor-principle-component-analysis"><a name="marylou"></a> Aukosh Jagannath: Algorithmic thresholds for tensor principle component analysis</h4>

<p>Consider the problem of recovering a rank 1 tensor of order k that has been subject to Gaussian noise. The log-likelihood for this problem is highly non-convex. It is information theoretically possible to recover the tensor with a finite number of samples via maximum likelihood estimation, however, it is expected that one needs a polynomially diverging number of samples to efficiently recover it. What is the cause of this large statistical–to–algorithmic gap? To study this question, we investigate the thresholds for efficient recovery for a simple family of algorithms, Langevin dynamics and gradient descent. We view this problem as a member of a broader class of problems which correspond to recovering a signal from a non-linear observation that has been perturbed by isotropic Gaussian noise. We propose a mechanism for success/failure of recovery of such algorithms in terms of the strength of the signal on the high entropy region of the initialization. Joint work with G. Ben Arous (NYU) and R. Gheissari (NYU).</p>

<h4 id="a-name-marylou-a-marylou-gabrie-entropy-and-mutual-information-in-models-of-deep-neural-networks"><a name="marylou"></a> Marylou Gabrie: Entropy and mutual information in models of deep neural networks</h4>

<p>The successes and the multitude of applications of deep learning methods have spurred efforts towards quantitative modeling of the performance of deep neural networks. In particular, an information-theoretic approach linking generalization capabilities to compression has been receiving increasing interest. Nevertheless, it is in practice computationally intractable to compute entropies and mutual informations in industry-sized neural networks. In this talk, we will consider instead a class of models of deep neural networks, for which an expression for these information-theoretic quantities can be derived from the replica method. We will examine how mutual informations between hidden and input variables can be reported along the training of such neural networks on synthetic datasets.
This work was done in collaboration with Andre Manoel (Owkin), Clément Luneau (EPFL), Jean Barbier (EPFL), Nicolas Macris (EPFL), Florent Krzakala (LPS ENS) and Lenka Zdeborova (IPHT CEA).</p>

<h4 id="a-name-anna-a-anna-seigal-structured-tensors-and-the-geometry-of-data"><a name="anna"></a> Anna Seigal: Structured Tensors and the Geometry of Data</h4>

<p>Abstract: Tensors are higher dimensional analogues of matrices; they are used to record data with multiple changing variables. Interpreting tensor data requires finding low rank structure, and the structure depends on the application or context. In this talk, we describe four projects in the study of structured tensors. Often tensors of interest define semi-algebraic sets, given by polynomial equations and inequalities. We give a characterization of the set of tensors of real rank two, and answer questions about statistical models using probability tensors and semi-algebraic statistics. We also study cubic surfaces as symmetric tensors, and describe work on learning a path from its three dimensional signature tensor. This talk is based on joint work with Guido Montúfar, Max Pfeffer, and Bernd Sturmfels.</p>

<h4 id="a-name-david-a-david-shirokoff-convex-relaxations-for-variational-problems-arising-from-models-of-self-assembly"><a name="david"></a>  David Shirokoff: Convex relaxations for variational problems arising from models of self-assembly</h4>

<p>We examine the problem of minimizing a class of nonlocal, nonconvex variational problems that arise from modeling a large number of pairwise interacting particles in the presence of thermal noise (i.e. molecular dynamics). Although finding and verifying local minima to these functionals is relatively straightforward, computing and verifying global minima is much more difficult. Global minima (ground states) are important as they characterize the structure of matter in models of self-assembly. We discuss how minimizing the functionals can be viewed as testing whether an associated bilinear form is co-positive. We then develop sufficient conditions for global optimality (which in some cases are provably sharp) obtained through a convex relaxation related to the cone of co-positive functionals.  The resulting convex relaxation results in a conic variational problem with an infinite number of Fourier constraints, and leads to a variety of computational challenges.  Pending time, we provide details on matrix-free interior point algorithms that alleviate some of the computational difficulties (i.e. solutions may be Dirac masses) associated with the large-scale problems.</p>

<h3 id="schedule-summer-18">Schedule Summer 18</h3>

<table>
<thead>
<tr>
<th>Date &amp; Time</th>
<th align="center">Speaker</th>
<th align="center">Title</th>
<th align="left">Room</th>
</tr>
</thead>

<tbody>
<tr>
<td>May 30, 10:15am</td>
<td align="center"><a href="http://yuguangwang.com/">Yu Guang Wang</a> (ICERM and The University of New South Wales, Sydney)</td>
<td align="center"><a href="#yu">Tight framelets and fast framelet filter bank transforms on manifolds</a></td>
<td align="left">WWH 317</td>
</tr>

<tr>
<td>June 6, 10:30am</td>
<td align="center"><a href="https://www.math.uci.edu/~rvershyn/index.html">Roman Vershynin</a> (UCI)</td>
<td align="center"><a href="#roman">From number theory to machine learning: a hunt for smooth Boolean functions</a></td>
<td align="left">WWH 317</td>
</tr>

<tr>
<td>June 13, 11:00am</td>
<td align="center"><a href="https://sites.google.com/site/aidakhajavirad/">Aida Khajavirad</a> (CMU and NYU)</td>
<td align="center"><a href="#aida">The multilinear polytope for acyclic Hypergraphs</a></td>
<td align="left">WWH 317</td>
</tr>

<tr>
<td>July 3, 10:30am</td>
<td align="center">Chiheon Kim (MIT)</td>
<td align="center"><a href="#kim">Statistical Limits of Graphical Channel Models</a></td>
<td align="left">WWH 317</td>
</tr>
</tbody>
</table>

<h3 id="abstracts-1">Abstracts</h3>

<h4 id="a-name-yu-a-yu-guang-wang-tight-framelets-and-fast-framelet-filter-bank-transforms-on-manifolds"><a name="yu"></a>  Yu Guang Wang: Tight framelets and fast framelet filter bank transforms on manifolds</h4>

<p>Data in practical application with some structure can be viewed as sampled
from a manifold, for instance, data on a graph and in astrophysics. A smooth and
compact Riemannian manifold M, including examples of spheres, tori, cubes and
graphs, is an important geometric struncture. In this work, we construct a type of
tight framelets using quadrature rules on M to represent the data (or a function)
and to exploit the derived framelets to process the data (for example, image and
signal processing on the sphere or graphs).</p>

<p>One critical computation for framelets is to compute, from the framelet coefficients for the input data (which are assumed at the highest level), the framelet coefficients at lower levels, and also to evaluate the function values at new nodes using the framelet representation. We design an efficient computational strategy, which we call fast framelet filter bank transform (FMT), to compute the framelet
coefficients and to recover the function. Assuming the fast Fourier transform (FFT) and using quadrature rules on the manifold M, the FMT has the same computational complexity as the FFT. Numerical examples illustrate the efficiency and accuracy of the algorithm for the framelets.
This is joint work with Q. T. Le Gia, Ian Sloan and Rob Womersley (UNSW Sydney), Houying Zhu (University of Melbourne) and Xiaosheng Zhuang (City University of Hong Kong).</p>

<h4 id="a-name-roman-a-roman-vershynin-from-number-theory-to-machine-learning-a-hunt-for-smooth-boolean-functions"><a name="roman"></a>  Roman Vershynin: From number theory to machine learning: a hunt for smooth Boolean functions</h4>

<p>The most fundamental kind of functions studied in computer science are Boolean functions. They take n bits as an input and return one bit as an output. Most Boolean functions oscillate a lot, which is analogous to the fact that &ldquo;most&rdquo; continuous functions on R are nowhere differentiable. If we want to generate a &ldquo;smooth&rdquo; Boolean function, we can take the sign of some polynomial of low degree in n variables. Such functions are called polynomial threshold functions, and they are widely used in machine learning as classification devices. Surprisingly, we do not know how many polynomial threshold functions there are with a given degree! Even an approximate answer to this question has been known only for polynomials of degree 1, i.e. for linear functions. In a very recent joint work with Pierre Baldi, we found a way to approximately count polynomial threshold functions of any fixed degree. This solves a problem of M. Saks that goes back to 1993 and earlier. Our argument draws ideas from analytical number theory, additive combinatorics, enumerative combinatorics, probability and discrete geometry. I will describe some of these connections, focusing particularly on a beautiful interplay of zeta and Mobius funcitons in number theory, hyperplane arrangements in enumerative combinatorics and random tensors in probability theory.</p>

<h4 id="a-name-yu-a-aida-khajavirad-the-multilinear-polytope-for-acyclic-hypergraphs"><a name="yu"></a>  Aida Khajavirad: The multilinear polytope for acyclic Hypergraphs</h4>

<p>We consider the multilinear polytope defined as the convex hull of the set of binary points satisfying a collection of multilinear equations. Such sets are of fundamental importance in many types of mixed-integer nonlinear optimization problems, such as binary polynomial optimization. Utilizing an equivalent hypergraph representation, we study the facial structure of the multilinear polytope in conjunction with the acyclicity degree of the underlying hypergraph. We derive various types of facet-defining inequalities and provide explicit characterizations for the multilinear polytope of Berge-acylic and gamma-acyclic hypergraphs. As an important byproduct, we present a new class of cutting planes for constructing stronger polyhedral relaxations of mixed-integer nonlinear optimization problems with multilinear sub-expressions.  Finally, we detail on the complexity of corresponding separation problems and embed the proposed cut generation algorithm at every node of the branch-and-reduce global solver BARON.  Extensive computational results will be presented.</p>

<h4 id="a-name-kim-a-chiheon-kim-statistical-limits-of-graphical-channel-models"><a name="kim"></a>  Chiheon Kim: Statistical Limits of Graphical Channel Models</h4>

<p>We investigate the exact recovery problem in graphical channel
models. Graphical channel model is defined as following: given a
hypergraph H=(V,E) and a hidden labeling x of V, we observe mutually
independent random values {y_e: e in E} where y_e is generated from a
distribution which only depends on the labels {x_u: u in e}. This model
encompasses many statistical models such as the stochastic block model,
spiked Gaussian tensor model, and sparse graph codes. We consider the
problem of exactly recovering the ground truth x from a sample y, and
prove that under mild conditions on the channel, it exhibits a sharp
phase transition behavior. Precisely, we find the explicit constant I
(depending on the channel) such that the exact recovery is achievable
w.h.p. if I &gt; 1 and it is not achievable if I &lt; 1. Joint work with
Afonso S. Bandeira and Michel X. Goemans.</p>

<h3 id="schedule-spring-18">Schedule Spring 18</h3>

<table>
<thead>
<tr>
<th>Date &amp; Time</th>
<th align="center">Speaker</th>
<th align="center">Title</th>
<th align="left">Room</th>
</tr>
</thead>

<tbody>
<tr>
<td>Jan 31, 10:15am</td>
<td align="center"><a href="https://www.simonsfoundation.org/team/mariano-tepper/">Mariano Tepper</a> (Simons Foundation)</td>
<td align="center"><a href="#tepper">Clustering is semidefinitely not that hard</a></td>
<td align="left">WWH 905</td>
</tr>

<tr>
<td>Feb 7, 10:15am</td>
<td align="center"><a href="https://cims.nyu.edu/~venturi/">Luca Venturi</a> (NYU)</td>
<td align="center"><a href="#venturi">Connectivity of Neural Networks Optimization Landscapes</a></td>
<td align="left">WWH 905</td>
</tr>

<tr>
<td>Feb 21, 10:15am</td>
<td align="center"><a href="https://web.math.princeton.edu/~jpereira/">Joao Pereira</a></td>
<td align="center"><a href="#morais">Estimation in multireference alignment and generalizations.</a></td>
<td align="left">WWH 905</td>
</tr>

<tr>
<td>Mar 7 11 am</td>
<td align="center"><a href="https://cims.nyu.edu/~sagun/">Levent Sagun</a></td>
<td align="center">Comparing Dynamics: Deep Neural Networks versus Glassy Systems</td>
<td align="left">WWH 905</td>
</tr>

<tr>
<td>Mar 27 1 pm</td>
<td align="center"><a href="https://www.math.u-psud.fr/~lavenant/">Hugo Lavenant</a></td>
<td align="center"><a href="#lavenant">Harmonic mappings valued in the Wasserstein space</a></td>
<td align="left">CDS C15</td>
</tr>

<tr>
<td>Apr 30 11 am</td>
<td align="center"><a href="http://www.mathc.rwth-aachen.de/en/~bouchot/">Jean-Luc Bouchot</a></td>
<td align="center"><a href="#bouchot">Compressed sensing Petrov-Galerkin: When data science helps solve problem in applied mathematics</a></td>
<td align="left">WWH 202</td>
</tr>
</tbody>
</table>

<hr />

<h3 id="abstracts-2">Abstracts</h3>

<h4 id="a-name-bouchot-a-jean-luc-bouchot-compressed-sensing-petrov-galerkin-when-data-science-helps-solve-problem-in-applied-mathematics"><a name="bouchot"></a>  Jean-Luc Bouchot: Compressed sensing Petrov-Galerkin: When data science helps solve problem in applied mathematics</h4>

<p>Motivated by problems in uncertainty quantification, we introduce a scheme for the uniform approximation of high-dimensional parametric PDEs. Exploiting an analytic dependence of certain PDEs in the parameters, allows to show some convergence rates for non-linear approximation.
Building on this remark, one computes (or has access to) independent snapshots of solutions for random parameters and use them in a weighted sparse recovery framework. This allows to approximate the solution map in a polynomial chaos in a number of snapshots that scales optimally (up to log factors) with the intrinsic sparsity of the solution. A further extension based on multi-level decomposition allows for efficient computation and can be shown to deliver uniform approximation (in the parameter space) in a computing time in the order of the approximation of a single solution.</p>

<h4 id="a-name-lavenant-a-hugo-lavenant-harmonic-mappings-valued-in-the-wasserstein-space"><a name="lavenant"></a>  Hugo Lavenant: Harmonic mappings valued in the Wasserstein space</h4>

<p>The Wasserstein space, which is the space of probability measures endowed with the so-called (quadratic) Wasserstein distance coming from optimal transport, can formally be seen as a Riemannian manifold of infinite dimension. We propose, through a variational approach, a definition of harmonic mappings defined over a domain of R^n and valued in the Wasserstein space. We will show how one can build a fairly satisfying theory which captures some key features of harmonicity and how it is related to the concepts of geodesics (the so-called McCann&rsquo;s interpolation) and barycenters in the Wasserstein space. Other than a better understanding of the Wasserstein space, the motivation of such a study can be found in geometric data analysis.</p>

<h4 id="a-name-morais-a-joao-pereira-estimation-in-multireference-alignment-and-generalizations"><a name="morais"></a> Joao Pereira: Estimation in multireference alignment and generalizations</h4>

<p>In the multireference alignment model, a signal is observed by the action of a random circular translation and the addition of Gaussian noise. Of particular interest is the sample complexity, i.e., the number of observations/samples needed in terms of the signal-to-noise ratio (SNR), the signal energy divided by the noise variance, in order to drive the mean-square error to zero. Previous work showed that if the translations are drawn from the uniform distribution, then, in the low SNR regime, the sample complexity of the problem scales as $\omega(1/\SNR^3)$.  We show, that if however the translation distribution is aperiodic, the sample complexity in the same regime drops down to $\omega(1/\SNR^2)$. This rate is achieved by a simple spectral algorithm. The lower bound follows from a generalization of the Chapman-Robbins bound for orbits and an expansion of the $\chi^2$ divergence at low SNR, and can be generalized for general group actions and projections. This suggests the method of moments is optimal in the low SNR regime.</p>

<p>Joint work with Emmanuel Abbe, Tamir Bendory, William Leeb, Nir Sharon and Amit Singer.</p>

<h4 id="a-name-venturi-a-luca-venturi-connectivity-of-neural-networks-optimization-landscapes"><a name="venturi"></a> Luca Venturi: Connectivity of Neural Networks Optimization Landscapes</h4>

<p>We study connectivity of sub-level sets of the square loss function of two-layers neural networks. This property implies abscence of poor local minima.
In particular, we explore the hypothesis that overparametrisation convexifies the functional space of neural network architectures.
I will start by extending the existing results about non-existence of bad local minima in the optimization of linear neural networks.
We then move to study non-linear activations using Reproducing Kernel Hilbert Spaces, deriving general results that include Empirical Risk Minimization.<br />
In the rest of the talk, I will focus on quadratic activations, and I will show how we can significatively improve the general RKHS bounds by exploiting the particular geometry of positive semidefinite matrices.
I will conclude by discussing some directions of possible future exploration.
Joint work with: A. Bandeira, J. Bruna.</p>

<h4 id="a-name-tepper-a-mariano-tepper-clustering-is-semidefinitely-not-that-hard"><a name="tepper"></a> Mariano Tepper: Clustering is semidefinitely not that hard</h4>

<p>In recent years, semidefinite programs (SDP) have been the subject of interesting research in the field of clustering. In many cases, these convex programs deliver the same answers as non-convex alternatives and come with a guarantee of optimality.
In this talk, I will argue that SDP-KM, a popular semidefinite relaxation of K-means, can learn manifolds present in the data, something not possible with the original K-means formulation. To build an intuitive understanding of SDP-KM&rsquo;s manifold learning capabilities, I will present a theoretical analysis on an idealized dataset. Additionally, SDP-KM even segregates linearly non-separable manifolds. As generic SDP solvers are slow on large datasets, I will present a new, convex and yet efficient, solver for SDP-KM. Our results render SDP-KM a versatile, understandable, and powerful tool for manifold learning.</p>

<h3 id="schedule-spring-17">Schedule Spring 17</h3>

<table>
<thead>
<tr>
<th>Date &amp; Time</th>
<th align="center">Speaker</th>
<th align="center">Title</th>
<th align="left">Room</th>
</tr>
</thead>

<tbody>
<tr>
<td>Mar 21, 2:30pm</td>
<td align="center"><a href="http://www-personal.umich.edu/~erebrova/index.html">Liza Rebrova</a> (U Michigan)</td>
<td align="center"><a href="#rebrova">Local and global obstructions for the random matrix norm regularization</a></td>
<td align="left">CDS 650</td>
</tr>

<tr>
<td></td>
<td align="center"></td>
<td align="center"></td>
<td align="left"></td>
</tr>
</tbody>
</table>

<hr />

<h3 id="abstracts-3">Abstracts</h3>

<h4 id="a-name-rebrova-a-liza-rebrova-local-and-global-obstructions-for-the-random-matrix-norm-regularization"><a name="rebrova"></a> Liza Rebrova: Local and Global obstructions for the random matrix norm regularization</h4>

<p>We study large n by n random matrices A with i.i.d. entries. If the distribution of the entries have mean zero and at least gaussian decay, then the operator norm ||A|| is at most of order sqrt(n) with high probability. However, for the distributions with heavier tails we cannot expect the same norm bound any more. So, we are motivated by the question: under what conditions operator norm of a heavy-tailed matrix can be improved by modifying just a small fraction of its entries (a small sub-matrix of A)? I will explain why this happens exactly when the entries of A have zero mean and bounded variance. I will also discuss the almost optimal dependence between the size of the removed sub-matrix and the resulting operator norm that we&rsquo;ve obtained. This is a joint work with Roman Vershynin, inspired by the methods developed recently by Can Le and R. Vershynin and in our joint work with Konstantin Tikhomirov.
<strong>Room:</strong> Center for Data Science, NYU, 60 5th ave, room 650
<strong>Time:</strong> 2:30pm-3:30pm.</p>

			</div>

			
		</div>

  </body>
</html>
