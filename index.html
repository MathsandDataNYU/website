<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.104.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>the MAD Seminar &middot; Math and Data</title>

  
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="https://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="https://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
      
         
                <li>
                    <a href="/people/corefaculty/">
                        
                        <span>Faculty</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/phds/">
                        
                        <span>PhD/MsC Students</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/postdocs/">
                        
                        <span>Postdocs and Fellows</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/reading/">
                        
                        <span>Reading Groups</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/thanks/">
                        
                        <span>Sponsors</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/seminar/">
                        
                        <span>the MAD Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/madplus/">
                        
                        <span>the MAD&#43; Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/micsem/">
                        
                        <span>the MIC Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/miscpeople/">
                        
                        <span>Visitors and Alumni</span>
                    </a>
                </li>
        
      
    </ul>

  </div>
</div>



    <main class="content container">
    <div class="post">
  <h1>the MAD Seminar</h1>
  <p>The MaD seminar features leading specialists at the interface
of Applied Mathematics, Statistics and Machine Learning. It is partly supported by the Moore-Sloan Data Science Environment at NYU.</p>
<p><strong>Room:</strong> Auditorium Hall 150, Center for Data Science, NYU, <a href="https://www.google.com/maps/place/NYU+Center+for+Data+Science/@40.735016,-73.9969907,17z/data=!3m1!4b1!4m5!3m4!1s0x89c2599787834ad9:0x5dd8af15d9fbc8a3!8m2!3d40.735016!4d-73.994802">60 5th ave</a>.</p>
<p><strong>Time:</strong> 2:00pm-3:00pm</p>
<p><strong>Subscribe to the Seminar Mailing list <a href="http://cims.nyu.edu/mailman/listinfo/mad">here</a></strong></p>
<h3 id="schedule-with-confirmed-speakers">Schedule with Confirmed Speakers</h3>
<table>
<thead>
<tr>
<th style="text-align:center">Date</th>
<th style="text-align:center">Speaker</th>
<th style="text-align:center">Title</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">February 9</td>
<td style="text-align:center"><a href="http://www.columbia.edu/~cgr2130/">Cynthia Rush (Columbia)</a></td>
<td style="text-align:center"><a href="#rush">Exact Asymptotics with Approximate Message Passing and a Study of the Type 1-Type 2 Error Trade-off for SLOPE</a></td>
</tr>
<tr>
<td style="text-align:center">February 16</td>
<td style="text-align:center"><a href="https://sdean.website/">Sarah Dean (Cornell)</a></td>
<td style="text-align:center"><a href="#sarah">Participation Dynamics in Learning Systems</a></td>
</tr>
<tr>
<td style="text-align:center">February 23</td>
<td style="text-align:center"><a href="https://menard.pha.jhu.edu/">Brice Ménard (JHU)</a></td>
<td style="text-align:center"><a href="#brice">Opening the neural network black box</a></td>
</tr>
<tr>
<td style="text-align:center">March 16</td>
<td style="text-align:center"><a href="https://maxim.ece.illinois.edu/">Maxim Raginsky (UIUC)</a></td>
<td style="text-align:center"><a href="#maxim">Variational Principles for Mirror Descent and Mirror Langevin Dynamics</a></td>
</tr>
<tr>
<td style="text-align:center">March 23</td>
<td style="text-align:center"><a href="https://web.stanford.edu/~jblanche/">Jose Blanchet (Stanford)</a></td>
<td style="text-align:center"><a href="#jose">A Unified View of Distributional Robust Decision Making</a></td>
</tr>
<tr>
<td style="text-align:center">March 30</td>
<td style="text-align:center"><a href="https://jasondlee88.github.io/">Jason Lee (Princeton)</a></td>
<td style="text-align:center"><a href="#jdl">Feature Learning with SGD</a></td>
</tr>
<tr>
<td style="text-align:center">April 6</td>
<td style="text-align:center">Surbhi Goel</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">April 13</td>
<td style="text-align:center">Lenka Zdeborova</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">April 20</td>
<td style="text-align:center">Andre Wibisono</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">April 27</td>
<td style="text-align:center">Jean Ponce</td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2022/">Schedule Fall 2022</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2022/">Schedule Spring 2022</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2021/">Schedule Fall 2021</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2020/">Schedule Spring 2020</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2019/">Schedule Fall 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2019/">Schedule Spring 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2018/">Schedule Fall 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2018/">Schedule Spring 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2017/">Schedule Fall 2017</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2017/">Schedule Spring 2017</a></p>
<h3 id="abstracts">Abstracts</h3>
<h4 id="a-namerusha-cynthia-rush-exact-asymptotics-with-approximate-message-passing-and-a-study-of-the-type-1-type-2-error-trade-off-for-slope"><a name='rush'></a> Cynthia Rush: Exact Asymptotics with Approximate Message Passing and a Study of the Type 1-Type 2 Error Trade-off for SLOPE</h4>
<p>Approximate message passing (AMP) is a class of iterative algorithms that can be used to systematically derive exact expressions for the asymptotic risk and other performance metrics for estimators that are constructed as solutions to a broad class of convex optimization problems. In this talk, we present a general program for using AMP in this way and we provide a specific example by using this approach to study the asymptotic model selection properties of sorted L1 penalized estimation (SLOPE). Sorted L1 regularization has been incorporated into many methods for solving high-dimensional statistical estimation problems, including using SLOPE in the context of linear regression. We will show how this regularization technique improves variable selection relative to the LASSO by characterizing the optimal SLOPE trade-off between the false discovery proportion and true positive proportion or, equivalently, between measures of type I and type II error. Collaborators on this work include Zhiqi Bu, Jason Klusowski, and Weijie Su (<a href="https://arxiv.org/abs/1907.07502">https://arxiv.org/abs/1907.07502</a> and <a href="https://arxiv.org/abs/2105.13302">https://arxiv.org/abs/2105.13302</a>) and Oliver Feng, Ramji Venkataramanan, and Richard Samworth (<a href="https://arxiv.org/abs/2105.02180)">https://arxiv.org/abs/2105.02180)</a>.</p>
<h4 id="a-namesaraha-sarah-dean-participation-dynamics-in-learning-systems"><a name='sarah'></a> Sarah Dean: Participation Dynamics in Learning Systems</h4>
<p>The choice to participate in a data-driven system, often made on the basis of the quality of that system, influences the ability of the system to learn and improve. Participation choices manifest as distribution shifts which are partially endogeneous, i.e. caused by the machine learning system itself. In this talk, I will discuss participation dynamics in the presence of multiple learners. We introduce and study a general class of loss-reducing dynamics, in which learners retrain to improve predictive performance and users shift participation towards better performing learners. We characterize the stable equilibria and discuss the implications in terms of social welfare and fairness. Based on joint work with Mihaela Curmei, Maryam Fazel, Jamie Morgenstern, and Lillian Ratliff.</p>
<h4 id="a-namebricea-brice-menard-opening-the-neural-network-black-box"><a name='brice'></a> Brice Menard: Opening the neural network black box</h4>
<p>I will present a simple point of view allowing us to make sense of the weights in a trained neural network. I will show how to characterize what has been learned, extract quasi-sufficient summary statistics, and use them to generate new networks performing well without any training. I will show that the symmetry group of neural networks are layer-based rotations. When taken into account network weights always converge to the same solution. I will illustrate these results using standard classification tasks on CIFAR-10 and ImageNet and I will introduce a model that captures all these properties. Finally, I will show that most of the stochasticity inherent to neural networks and their training is largely negligible. Collaborators: F. Guth, S. Mallat &amp; G. Rochette.</p>
<h4 id="a-namemaxima-maxim-raginsky-variational-principles-for-mirror-descent-and-mirror-langevin-dynamics"><a name='maxim'></a> Maxim Raginsky: Variational Principles for Mirror Descent and Mirror Langevin Dynamics</h4>
<p>Mirror descent, introduced by Nemirovsky and Yudin in the 1970s, is a primal-dual convex optimization method that can be tailored to the geometry of the optimization problem at hand through the choice of a strongly convex distance-generating potential function. It arises as a basic primitive in a variety of applications, including large-scale optimization, machine learning, and control. In this talk, based on joint work with Belinda Tzen, Anant Raj, and Francis Bach, I will discuss a variational formulation of mirror descent and of its stochastic variant, mirror Langevin dynamics. The main idea, inspired by classic work of Brezis and Ekeland, is to show that mirror descent emerges as a closed-loop solution for a certain optimal control problem, and the Bellman value function is given by the dual-space Bregman divergence between the initial condition and the global minimizer of the objective function. This formulation has several interesting corollaries and implications, including a form of implicit regularization, which I will discuss.</p>
<h4 id="a-namejosea-jose-blanchet-a-unified-view-of-distributional-robust-decision-making"><a name='jose'></a> Jose Blanchet: A Unified View of Distributional Robust Decision Making</h4>
<p>The goal of distributionally robust decision-making is to make reasonably good decisions under uncertainty when (in machine learning language) the “training” environment is different from the deployment environment. This is common in situations involving highly non-stationary environments or situations in which training needs to be done in a simulated environment due to various costs or limited information. DRO formulations are based on min-max games in which the manager plays a game against a fictitious adversary that is introduced to perform a disciplined “what-if-my-model-is-wrong” analysis. This approach has a rich tradition in the economics and control literature. Basically, there are two types of ways in which a probabilistic model can be wrong: a) the likelihoods are incorrect, or b) the actual outcomes are misspecified (or both). These types of models of misspecification have traditionally been treated separately (since the 80s) leading to various divergence formulations, contamination models and, more recently, optimal transport perturbations. The talk focuses on studying all of these from a unified standpoint via the theory of optimal transport with martingale constraints. We recover most forms of DRO formulations (and introduce new ones) using this approach, including some of the statistical analysis done in data-driven DRO. Further implications for dynamic decision-making under uncertainty are also explored.</p>
<p>This talk is based on joint work with Nick Bambos, Daniel Kuhn, Jiajin Li, Sirui Lin, Kyriakos Lotidis, and Bahar Tahksen.</p>
<h4 id="a-namejdla-jason-lee-feature-learning-with-sgd"><a name='jdl'></a> Jason Lee: Feature Learning with SGD</h4>
<p>I will present two recent works on analyzing feature learning for SGD. The first focuses on the task of learning a single index model \sigma(w* x)$ with respect to the isotropic Gaussian distribution in d dimensions, including the special case when \sigma is a kth order Hermite which corresponds to the Gaussian analog of parity learning. Prior work has shown that the sample complexity of learning w* is governed by the \emph{information exponent} k* of the link function \sigma, which is defined as the index of the first nonzero Hermite coefficient of \sigma. Prior upper bounds have shown that n &gt; d^{k*-1} samples suffice for learning w* and that this is tight for online SGD (Ben Arous et al., 2020). However, the CSQ lower bound for gradient based methods only shows that n &gt; d^{k*/2}$ samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on the Gaussian-smoothed loss learns w* with n &gt; d^{k*/2}$ samples.</p>
<p>Next, we turn to the problem of learning multi index models f(x) = g(Ux), where U encodes a latent representation of low dimension. Significant prior work has established that neural networks trained by gradient descent behave like kernel methods, despite significantly worse empirical performance of kernel methods. However, in this work we demonstrate that for this large class of functions that there is a large gap between kernel methods and gradient descent on a two-layer neural network, by showing that gradient descent learns representations relevant to the target task. We also demonstrate that these representations allow for efficient transfer learning, which is impossible in the kernel regime. Specifically, we consider the problem of learning polynomials which depend on only a few relevant directions, i.e. of the form f*(x)=g(Ux) where U is d by r. When the degree of f* is p, it is known that n≍d^p samples are necessary to learn f* in the kernel regime. Our primary result is that gradient descent learns a representation of the data which depends only on the directions relevant to f*. This results in an improved sample complexity of n≍d^2r+drp. Furthermore, in a transfer learning setup where the data distributions in the source and target domain share the same representation U but have different polynomial heads we show that a popular heuristic for transfer learning has a target sample complexity independent of d.</p>
<p>This is joint work with Alex Damian, Rong Ge,  Eshaan Nichani, and Mahdi Soltanolkotabi.</p>

</div>


    </main>

    
  </body>
</html>
