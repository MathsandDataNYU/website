<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.68.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>the MAD&#43; Seminar &middot; Math and Data</title>

  
  <link type="text/css" rel="stylesheet" href="http://mad.cds.nyu.edu/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="http://mad.cds.nyu.edu/css/poole.css">
  <link type="text/css" rel="stylesheet" href="http://mad.cds.nyu.edu/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="http://mad.cds.nyu.edu/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="http://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="http://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
      
         
                <li>
                    <a href="/about/">
                        
                        <span>About</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/corefaculty/">
                        
                        <span>Faculty</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/phds/">
                        
                        <span>PhD/MsC Students</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/postdocs/">
                        
                        <span>Postdocs and Fellows</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/reading/">
                        
                        <span>Reading Groups</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/seminar/">
                        
                        <span>the MAD Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/madplus/">
                        
                        <span>the MAD&#43; Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/micsem/">
                        
                        <span>the MIC Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/miscpeople/">
                        
                        <span>Visitors and Alumni</span>
                    </a>
                </li>
        
      
    </ul>

  </div>
</div>



    <main class="content container">
    <div class="post">
  <h1>the MAD&#43; Seminar</h1>
  <p>We are excited to announce the MaD+ (math and data plus) seminar, jointly organized between NYU and <a href="https://people.math.ethz.ch/~abandeira/">ETH</a>. It is now a virtual seminar series, and depending on logistics and interest potentially evolve to a multi-location seminar once the world returns to normal (with talks hosted in one of the locations, and streamed). The topics will be in line with the physical MaD seminar hosted at the Center for Data Science, which features leading specialists at the interface
of Applied Mathematics, Statistics and Machine Learning. We took inspiration from the fantastic virtual seminar <a href="https://sites.google.com/site/plustcs/">TCS+</a> from our friends in Theoretical Computer Science.</p>
<p>This semester it runs on Wednesdays in two time slots (each week one time slot) 4pm CET (10am EST) and and 2pm EST (8pm CET), in an attempt to accommodate the various working hours of more researchers around the world. Stay tuned for more great names that will be speaking this semester!</p>
<p>MaD seminars are recorded and streamed live. Links to the videos are available below.
You can subscribe to a calendar <a href="https://math.ethz.ch/s/math-and-data">here</a>.</p>
<h3 id="schedule-with-confirmed-speakers">Schedule with Confirmed Speakers</h3>
<table>
<thead>
<tr>
<th>Date</th>
<th align="center">Speaker</th>
<th align="center">Title</th>
<th align="center">Stream</th>
</tr>
</thead>
<tbody>
<tr>
<td>3/25, 10am EST</td>
<td align="center"><a href="https://www.jonathannilesweed.com">Jon Niles-Weed</a> (NYU)</td>
<td align="center"><a href="#jon">Matrix Concetration for Products</a></td>
<td align="center"><a href="https://ethz.zoom.us/j/631698662">Zoom Meeting</a></td>
</tr>
<tr>
<td>4/1, 10am EST</td>
<td align="center"><a href="https://web.math.princeton.edu/~weinan/">Weinan E</a> (Princeton)</td>
<td align="center"><a href="#weinan">A Mathematical Perspective of Machine Learning</a></td>
<td align="center"><a href="https://ethz.zoom.us/j/776255333">Zoom Meeting</a></td>
</tr>
<tr>
<td>4/8, 10am EST</td>
<td align="center"><a href="http://www-math.mit.edu/~rigollet/">Philippe Rigollet</a> (MIT)</td>
<td align="center"><a href="#rigollet">Statistical and Computational aspects of Wasserstein Barycenters</a></td>
<td align="center"><a href="https://ethz.zoom.us/j/278020112">Zoom Meeting</a></td>
</tr>
<tr>
<td>4/22, 10am EST</td>
<td align="center"><a href="http://www.mit.edu/~gamarnik/home.html">David Gamarnik</a> (MIT)</td>
<td align="center"><a href="#gamarnick">Overlap Gap Property: a Provable Barrier to Fast Optimization in Probabilistic Combinatorial Structures</a></td>
<td align="center"><a href="">Zoom Meeting</a></td>
</tr>
<tr>
<td>4/29, 10am EST</td>
<td align="center"><a href="https://people.math.ethz.ch/~vsara/">Sara Van de Geer</a> ETH Zurich</td>
<td align="center"><a href="#sara">Total Variation Regularization</a></td>
<td align="center"><a href="https://ethz.zoom.us/j/96038993793">Zoom</a></td>
</tr>
<tr>
<td>TBA</td>
<td align="center"><a href="https://statweb.stanford.edu/~candes/">Emmanuel Candes</a> (Stanford)</td>
<td align="center">TBA</td>
<td align="center"></td>
</tr>
<tr>
<td>5/20, 10am EST</td>
<td align="center"><a href="https://www.di.ens.fr/~fbach/">Francis Bach</a> (INRIA/ENS)</td>
<td align="center">TBA</td>
<td align="center"></td>
</tr>
<tr>
<td>5/27</td>
<td align="center"><a href="http://artax.karlin.mff.cuni.cz/~zdebl9am/index.htm">Lenka Zdeborova</a> (CNRS)</td>
<td align="center">TBA</td>
<td align="center"></td>
</tr>
<tr>
<td>6/3, 10am EST</td>
<td align="center"><a href="https://math.duke.edu/people/ingrid-daubechies">Ingrid Daubechies</a> (Duke)</td>
<td align="center">TBA</td>
<td align="center"></td>
</tr>
</tbody>
</table>
<h3 id="abstracts">Abstracts</h3>
<h4 id="a-namejona-jon-niles-weed-matrix-concentration-for-products"><a name="jon"></a> Jon Niles-Weed: Matrix Concentration for Products</h4>
<p>We develop nonasymptotic concentration bounds for products of independent random matrices. Such products arise in the study of stochastic algorithms, linear dynamical systems, and random walks on groups. Our bounds exactly match those available for scalar random variables and continue the program, initiated by Ahlswede-​Winter and Tropp, of extending familiar concentration bounds to the noncommutative setting. Our proof technique relies on geometric properties of the Schatten trace class. Joint work with D. Huang, J. A. Tropp, and R. Ward.</p>
<h4 id="a-nameweinana-weinan-e-a-mathematical-perspective-of-machine-learning"><a name="weinan"></a> Weinan E: A Mathematical Perspective of Machine Learning</h4>
<p>The heart of modern machine learning is the approximation of high dimensional functions. Traditional approaches, such as approximation by piecewise polynomials, wavelets, or other linear combinations of fixed basis functions, suffer from the curse of dimensionality. We will discuss representations and approximations that overcome this difficulty, as well as gradient flows that can be used to find the optimal approximation. We will see that at the continuous level, machine learning can be formulated as a series of reasonably nice variational and PDE-​like problems. Modern machine learning models/algorithms, such as the random feature and shallow/deep neural network models, can be viewed as special discretizations of such continuous problems. At the theoretical level, we will present a framework that is suited for analyzing machine learning models and algorithms in high dimension, and present results that are free of the curse of dimensionality. Finally, we will discuss the fundamental reasons that are responsible for the success of modern machine learning, as well as the subtleties and mysteries that still remain to be understood</p>
<h4 id="a-namerigolleta-philippe-rigollet-statistical-and-computational-aspects-of-wasserstein-barycenters"><a name="rigollet"></a> Philippe Rigollet: Statistical and Computational aspects of Wasserstein Barycenters</h4>
<p>The notion of average is central to most statistical methods. In this talk we study a generalization of this notion over the non-​Euclidean space of probability measures equipped with a certain Wasserstein distance. This generalization is often called Wasserstein Barycenters and empirical evidence suggests that these barycenters allow to capture interesting notions of averages in graphics, data assimilation and morphometrics. However the statistical (rates of convergence) and computational (efficient algorithms) for these Wasserstein barycenters are largely unexplored. The goal of this talk is to review two recent results: 1. Fast rates of convergence for empirical barycenters in general geodesic spaces, and, 2. Provable guarantees for gradient descent and stochastic gradient descent to compute Wasserstein barycenters. Both results leverage geometric aspects of optimal transport. Based on joint works (arXiv:1908.00828, arXiv:2001.01700) with Chewi, Le Gouic, Maunu, Paris, and Stromme.</p>
<h4 id="a-namegamarnicka-david-gamarnik-overlap-gap-property-a-provable-barrier-to-fast-optimization-in-probabilistic-combinatorial-structures"><a name="gamarnick"></a> David Gamarnik: Overlap Gap Property: a Provable Barrier to Fast Optimization in Probabilistic Combinatorial Structures</h4>
<p>Many combinatorial optimization problems defined on random instances exhibit an apparent gap between the optimal values, which can be computed by non-constructive means, and the best values achievable by fast (polynomial time) algorithms. Through a combined effort of mathematicians, computer scientists and statistical physicists, it became apparent that a potential barrier for designing fast algorithms bridging this gap is an intricate topology of nearly optimal solutions, in particular the presence of the Overlap Gap Property (OGP), which we will introduce in this talk. We will discuss how for many such problems the onset of the OGP phase transition introduces indeed a provable barrier to a broad class of polynomial time algorithms. Examples of such problems include the problem of finding a largest independent set of a random graph, finding a largest cut in a random hypergrah, the problem of finding a ground state of a p-spin model, and also many problems in high-dimensional statistics field. In this talk we will demonstrate in particular why OGP is a barrier for three classes of algorithms designed to find a near ground state in p-spin models arising in the field of spin glass theory: Approximate Message Passing algorithms, algorithms based on low-degree polynomial and Langevin dynamics.
Joint work with Aukosh Jagannath and Alex Wein</p>
<h4 id="a-namesaraa-sara-van-de-geer-total-variation-regularization"><a name="sara"></a> Sara van de Geer: Total variation Regularization</h4>
<p>Let Y be a n-dimensional vector of independent observations with unknown mean f^0 := E Y. We consider the estimator f_D that solves the ``analysis problem&rdquo; min_f {|| Y - f ||_2^2/ n + 2 lambda || D f ||_1 } , where D is a given , m times n matrix and lambda&gt;0 is a tuning parameter. An example for the matrix D is the (first order) difference operator (Df )<em>i= f</em>{i}- f_{i-1} , \ i \in [2:n] in which case || Df ||_1 = TV(f) is the (first order) total variation of the vector f. Other examples include higher order discrete derivatives, total variation on graphs and total variation in higher dimensions. Our aim is to show that the estimator f_D is adaptive. For example, when f^0 is a piecewise linear function, we show that the analysis estimator f_D, with D the second order differences operator, adapts to the number of kinks of f^0. As is the case with the Lasso, the theory for the analysis estimator f_D requires a form of &ldquo;restricted eigenvalue&rdquo; condition. We will show that this can be established using interpolating vectors. We will illustrate this (with drawings) for the various examples.</p>

</div>


    </main>

    
  </body>
</html>
