<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="//gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.88.1" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>the MAD Seminar spring 2020 &middot; Math and Data</title>

  
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://mad.cds.nyu.edu/css/hyde.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  
</head>

  <body class=" ">
  <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <IMG SRC="https://mad.cds.nyu.edu//img/logo2.png" ALT="some text" WIDTH=400 HEIGHT=100>
      <a href="https://mad.cds.nyu.edu/"><h1>Math and Data</h1></a>
      <p class="lead">
       Center for Data Science and Courant Institute, NYU 
      </p>
    </div>

    <ul class="sidebar-nav">
      
      
      
         
                <li>
                    <a href="/about/">
                        
                        <span>About</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/corefaculty/">
                        
                        <span>Faculty</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/phds/">
                        
                        <span>PhD/MsC Students</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/postdocs/">
                        
                        <span>Postdocs and Fellows</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/reading/">
                        
                        <span>Reading Groups</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/thanks/">
                        
                        <span>Sponsors</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/seminar/">
                        
                        <span>the MAD Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/madplus/">
                        
                        <span>the MAD&#43; Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/micsem/">
                        
                        <span>the MIC Seminar</span>
                    </a>
                </li>
        
      
         
                <li>
                    <a href="/people/miscpeople/">
                        
                        <span>Visitors and Alumni</span>
                    </a>
                </li>
        
      
    </ul>

  </div>
</div>



    <main class="content container">
    <div class="post">
  <h1>the MAD Seminar spring 2020</h1>
  <p>The MaD seminar features leading specialists at the interface
of Applied Mathematics, Statistics and Machine Learning. It is partly supported by the Moore-Sloan Data Science Environment at NYU.</p>
<p>MaD seminars are recorded and streamed live. Links to the videos are available below.</p>
<p><strong>Room:</strong> Auditorium Hall 150, Center for Data Science, NYU, <a href="https://www.google.com/maps/place/NYU+Center+for+Data+Science/@40.735016,-73.9969907,17z/data=!3m1!4b1!4m5!3m4!1s0x89c2599787834ad9:0x5dd8af15d9fbc8a3!8m2!3d40.735016!4d-73.994802">60 5th ave</a>.</p>
<p><strong>Time:</strong> 2:00pm-3:00pm, Reception will follow.</p>
<p><strong>Subscribe to the Seminar Mailing list <a href="http://cims.nyu.edu/mailman/listinfo/mad">here</a></strong></p>
<h3 id="schedule-with-confirmed-speakers">Schedule with Confirmed Speakers</h3>
<table>
<thead>
<tr>
<th>Date</th>
<th style="text-align:center">Speaker</th>
<th style="text-align:center">Title</th>
<th style="text-align:center">Live Stream</th>
</tr>
</thead>
<tbody>
<tr>
<td>Jan 23</td>
<td style="text-align:center">Paromita Dubey (UC Davis)</td>
<td style="text-align:center"><a href="#dubey">Fréchet Change Point Detection</a></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td>Jan 30</td>
<td style="text-align:center">Yaniv Romano (Stanford)</td>
<td style="text-align:center"><a href="#romano">Reliability, Equity, and Reproducibility in Modern Machine Learning</a></td>
<td style="text-align:center"><a href="https://nyursc.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=6a02c214-b87c-4d08-b089-ab4a012c6457">video</a></td>
</tr>
<tr>
<td>Feb 6</td>
<td style="text-align:center">Kaizheng Wang (Princeton)</td>
<td style="text-align:center"><a href="#wang">Latent variable models: spectral methods and non-convex optimization</a></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td>Feb 13</td>
<td style="text-align:center">Laure Zanna (NYU)</td>
<td style="text-align:center"><a href="#zanna">Blending machine learning and physics to improve climate modeling</a></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td>Feb 20</td>
<td style="text-align:center">Yash Deshpande (MIT)</td>
<td style="text-align:center"><a href="#deshpande">Two problems in modern statistical inference</a></td>
<td style="text-align:center"><a href="https://drive.google.com/file/d/1HkiuN--iKFzchSazRs9UUmyjyb8GifYu/view">video</a></td>
</tr>
<tr>
<td>Feb 27</td>
<td style="text-align:center">Rebecca Willett (UChicago)</td>
<td style="text-align:center"><a href="#willett">Learning to Solve Inverse Problems in Imaging</a></td>
<td style="text-align:center"><a href="https://nyursc.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=ed4a3e7d-1104-4b87-a21f-ab5a012d4f1e">video</a></td>
</tr>
<tr>
<td>Mar 5</td>
<td style="text-align:center">Stefanie Jegelka (MIT)</td>
<td style="text-align:center"><a href="#jegelka">Representation and Learning in Graph Neural Networks</a></td>
<td style="text-align:center"><a href="https://nyursc.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=29ab8b5e-ff0d-425f-95c7-ab5a012d992d">video</a></td>
</tr>
</tbody>
</table>
<hr>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2019/">Schedule Fall 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2019/">Schedule Spring 2019</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2018/">Schedule Fall 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2018/">Schedule Spring 2018</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_fall2017/">Schedule Fall 2017</a></p>
<p><a href="https://mathsanddatanyu.github.io/website/seminar_spring2017/">Schedule Spring 2017</a></p>
<hr>
<h3 id="abstracts">Abstracts</h3>
<h4 id="a-namejegelkaa-stefanie-jegelka-representation-and-learning-in-graph-neural-networks"><a name="jegelka"></a> Stefanie Jegelka: Representation and Learning in Graph Neural Networks</h4>
<p>Graph Neural Networks (GNNs) have become a popular tool for learning representations of graph-structured inputs, with applications in computational chemistry, recommendation, pharmacy, reasoning, and many other areas.
After a brief introduction to Graph Neural Networks, this talk will show recent results on representational power and learning in GNNs. First, we study the discriminative power of message passing type networks as a function of architecture choices, and, in the process, find that some popular architectures cannot learn to distinguish certain simple graph structures. Second, while many network architectures can represent a task, some learn it better than others. At the example of reasoning tasks, We formalize the interaction of the network architecture and the structure of the task, and probe its effect on learning. Third, we analyze learning via new generalization bounds for GNNs.</p>
<p>This talk is based on joint work with Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken-ichi Kawarabayashi, Weihua Hu, Jure Leskovec, Vikas Garg and Tommi Jaakkola.</p>
<h4 id="a-namewilletta-rebecca-willett-learning-to-solve-inverse-problems-in-imaging"><a name="willett"></a> Rebecca Willett: Learning to Solve Inverse Problems in Imaging</h4>
<p>Many challenging image processing tasks can be described by an
ill-posed linear inverse problem: deblurring, deconvolution,
inpainting, compressed sensing, and superresolution all lie in this
framework. Traditional inverse problem solvers minimize a cost
function consisting of a data-fit term, which measures how well an
image matches the observations, and a regularizer, which reflects
prior knowledge and promotes images with desirable properties like
smoothness. Recent advances in machine learning and image processing
have illustrated that it is often possible to learn a regularizer from
training data that can outperform more traditional regularizers.
However, some popular approaches are highly suboptimal in terms of
sample complexity, which we can see from the perspective of
conditional density estimation. I will describe an end-to-end,
data-driven method of solving inverse problems inspired by the Neumann
series, called a Neumann network. The Neumann network architecture
outperforms traditional inverse problem solution methods, model-free
deep learning approaches and state-of-the-art unrolled iterative
methods on standard datasets. Finally, when the images belong to a
union of subspaces and under appropriate assumptions on the forward
model, we prove there exists a Neumann network configuration that
well-approximates the optimal oracle estimator for the inverse problem
and demonstrate empirically that the trained Neumann network has the
form predicted by theory. This is joint work with Davis Gilton and
Greg Ongie.</p>
<h4 id="a-namedeshpandea-yash-deshpande-two-problems-in-modern-statistical-inference"><a name="deshpande"></a> Yash Deshpande: Two problems in modern statistical inference</h4>
<p>Rigorously and robustly quantifying uncertainty in modern data analysis settings is an outstanding open challenge. In this talk I will consider two reasons why  The first is the use of non-linear, and often optimization-based estimators necessitated by high dimensional data. The second is that data collection is often adaptive – the insight gleaned from prior data informs and influences the data collected in the future. These non-trivial correlations in even simple estimates complicate and invalidate conclusions from classical statistical theory.</p>
<p>The first part of this talk will center on contextual stochastic block models, as a canonical example of a high-dimensional estimation problem. I will demonstrate how ideas from statistical physics allow to obtain optimal estimation and testing results, via a sharp quantification of underlying uncertainty.</p>
<p>In the second part of the talk, I will focus on batched bandit algorithms, as exemplar of adaptively collected data. I will discuss online debiasing, an algorithmic procedure that ‘debiases’ estimators. Following debiasing, uncertainty measures like confidence intervals and p-values can be obtained straightforwardly.</p>
<h4 id="a-namezannaa-laure-zanna-blending-machine-learning-and-physics-to-improve-climate-modeling"><a name="zanna"></a> Laure Zanna: Blending machine learning and physics to improve climate modeling</h4>
<p>Numerical simulations used for weather and climate predictions solve approximations of the governing laws of fluid motions. The computational cost of these simulations limits the accuracy of the predictions. Uncertainties in the simulations and predictions ultimately originate from the poor or lacking representation of processes, such as turbulence, that are not resolved on the numerical grid of global climate models.
I will show that machine learning algorithms with imposed physical constraints are good candidates to improve the representation of processes that occur below the scales resolved by global models.
Specifically, I will propose new representations of ocean turbulence derived using relevance vector machines and convolutional neural networks trained on data from high-resolution idealized simulations.
The new models of turbulent processes are interpretable and/or encapsulate physics, and lead to improved simulations of the ocean.  Our results simultaneously open the door to the discovery of new physics from data and the improvement of numerical simulations of oceanic and atmospheric flows.</p>
<h4 id="a-namewanga-kaizheng-wang-latent-variable-models-spectral-methods-and-non-convex-optimization"><a name="wang"></a> Kaizheng Wang: Latent variable models: spectral methods and non-convex optimization</h4>
<p>Latent variable models lay the statistical foundation for data science problems with unstructured, incomplete and heterogeneous information. For the sake of computational efficiency, heuristic algorithms are proposed to extract the latent low-dimensional structures for downstream tasks. Despite their huge success in practice, theoretical understanding is lagging far behind and that hinders further advancement. In this talk, I will first show an L_p theory of eigenvector analysis that yields optimal recovery guarantees for spectral methods in many challenging problems. Then I will present a general framework for clustering based on non-convex optimization, and study its theoretical guarantees under statistical models. The results find applications in dimensionality reduction, mixture models, network analysis, recommendation systems, ranking and beyond.</p>
<h4 id="a-nameromanoa-yaniv-romano-reliability-equity-and-reproducibility-in-modern-machine-learning"><a name="romano"></a> Yaniv Romano: Reliability, Equity, and Reproducibility in Modern Machine Learning</h4>
<p>Modern machine learning algorithms have achieved remarkable performance in a myriad of applications, and are increasingly used to make impactful decisions in the hiring process, criminal sentencing, healthcare diagnostics and even to make new scientific discoveries. The use of data-driven algorithms in high-stakes applications is exciting yet alarming: these methods are extremely complex, often brittle, notoriously hard to analyze and interpret. Naturally, concerns have raised about the reliability, fairness, and reproducibility of the output of such algorithms. This talk introduces statistical tools that can be wrapped around any &ldquo;black-box&rdquo; algorithm to provide valid inferential results while taking advantage of their impressive performance. We present novel developments in conformal prediction and quantile regression, which rigorously guarantee the reliability of complex predictive models, and show how these methodologies can be used to treat individuals equitably. Next, we focus on reproducibility and introduce an operational selective inference tool that builds upon the knockoff framework and leverages recent progress in deep generative models. This methodology allows for reliable identification of a subset of important features that is likely to explain a phenomenon under-study in a challenging setting where the data distribution is unknown, e.g., mutations that are truly linked to changes in drug resistance.</p>
<h4 id="a-namedubeya-paromita-dubey-fréchet-change-point-detection"><a name="dubey"></a> Paromita Dubey: Fréchet Change Point Detection</h4>
<p>Change point detection is a popular tool for identifying locations in a data sequence where an abrupt change occurs in the data distribution and has been widely studied for Euclidean data. Modern data very often is non-Euclidean, for example distribution valued data or network data. Change point detection is a challenging problem when the underlying data space is a metric space where one does not have basic algebraic operations like addition of the data points and scalar multiplication.</p>
<p>In this talk, I propose a method to infer the presence and location of change points in the distribution of a sequence of independent data taking values in a general metric space. Change points are viewed as locations at which the distribution of the data sequence changes abruptly in terms of either its Fréchet mean or Fréchet variance or both.  The proposed method is based on comparisons of Fréchet variances before and after putative change point locations.  First, I will establish that under the null hypothesis of no change point the limit distribution of the proposed scan function is the square of a standardized Brownian Bridge.</p>
<p>It is well known that such convergence is rather slow in moderate to high dimensions. For more accurate results in finite sample applications, I will provide a theoretically justified bootstrap-based scheme for testing the presence of change points. Next, I will show that when a change point exists, (1) the proposed test is consistent under contiguous alternatives and (2) the estimated location of the change-point is consistent. All of the above results hold for a broad class of metric spaces under mild entropy conditions. Examples include the space of univariate probability distributions and the space of graph Laplacians for networks. I will illustrate the efficacy of the proposed approach in empirical studies and in real data applications with sequences of maternal fertility distributions. Finally, I will talk about some future extensions and other related research directions, for instance, when one has samples of dynamic metric space data. This talk is based on joint work with Prof. Hans-Georg Müller.</p>

</div>


    </main>

    
  </body>
</html>
